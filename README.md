![Banner](banner.png)

## About

Welcome to our comprehensive project on autoencoder archictectures, where start with introducing the motivations and purposes of autoencoder architectures. From there, we cover how to implement the (Vanilla) Autoencoder (AE), Variational Autoencoder (VAE), and finally, the more SOTA Vector-Quantized Variational Autoencoder (VQVAE). These implemntations are supplemented with an array of informative ablations, visualizations, and conceptual problems to give the learner a more complete understanding of the topic. Our "Where to Go Next" and "References" sections are great resources for additional learning. 

This resource was created as a final project for UC Berkeley's Deep Learning course [CS 182](https://inst.eecs.berkeley.edu/~cs182/).
