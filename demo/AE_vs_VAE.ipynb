{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXa4-u08T0rb"
   },
   "source": [
    "# Purpose and Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1xGKqxVUHT-"
   },
   "source": [
    "Welcome! In this notebook, we will be covering the basics of autoencoder architectures, in particular the (vanilla) autoencoder and the variatonal autoencoder architectures. Throughout the notebook, there will be a set of concept check questions. You can find the corresponding question in our supplemental conceptual guide.\n",
    "\n",
    "After completing this notebook, you will:\n",
    "\n",
    "\n",
    "1.   Have a complete understanding of the general AE and VAE architectures\n",
    "2.   Be able to implemting AEs and VAEs in Pytorch\n",
    "3.   Evaluate the models' performance (loss/accuracy), denoising ability, and latent space through visualizations\n",
    "\n",
    "*Note: It may be a good idea to review the basic principles of PCA as they will be referred to throughout the project*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFAbvDN8-zsV"
   },
   "source": [
    "## The Fundamental Setup of Autoencoder Architectures:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfsH6EqO-zsV"
   },
   "source": [
    "![AE.jpeg](img/AE.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">Fig 1. A diagram of the general (Vanilla) Autoencoder architecture. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z09G4GVz-zsW"
   },
   "source": [
    "As referenced in Hinton et. al., the purpose of autoencoders aligns very much with the motivations of PCA. One can consider the encoder to be a mapping from the high dimensional space of the input to the low-dimensional space of the latent representation. Similarly, the decoder *reconstructs* the low-dimensional represntation back into the original dimensionality. We discuss the purposes of this architecture in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75gnIpxG-zsW"
   },
   "source": [
    "### Concept Check 1.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKkb7_HU1545",
    "tags": []
   },
   "source": [
    "## Why Autoencoder Architectures? (Note to self: add images and figures later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBXPYnprV4wU"
   },
   "source": [
    "Autoencoder-style models have many different uses. We list several of the most notable ones here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUmatULcTZxr"
   },
   "source": [
    "1.   **Dimensionality Reduction:** As mentioned in Hinton et. al., the *main* motivation for the AE architecutere is to turn \"high-dimensional data\" into \"low dimensional codes\". Adopting from the spirit of PCA, the encoder of an autoencoder can be thought of as a *nonlinear* map of the input data into the compressed space and the decoder an analogous yet opposite map that reconstructs the latent representation into the same dimensionality as the original input. Given sufficient data and computation, AEs can outperform PCA because *AEs are not limited to learning a linear representation of the data*. Datasets that feature high-dimensionality (e.g. biological/genetic data) can benefit from first applying AE dimensionality reduction before applying a later downstream task to the data. In this way, AE architectures can serve as a preprocessing step for other tasks. Note: a more general term for this in research is called **representation learning**.\n",
    "2.   **Denoising:** Once again, similar to PCA, autoencoders can be utilized as preprocessing method for denoising data. Suppose we have an input vector $\\vec{x}_i$ and we add some noise $\\vec{\\epsilon}$ to it. Then our minimization problem for the autoencoder becomes $\\arg\\min_{f, g} \\sum_{i = 0}^{n}||\\vec{x}_i - g(f(\\vec{x}_i + \\vec{\\epsilon}))||^2$. From this objective, we see that an autoencoder used for denoising is motivated to ignore the noise.\n",
    "3.   **Data Generation:** After properly training an autoencoder architecture, one can ignore the encoder half of the model and sample from the latent representation (or code) distribution. This sampled point can be passed through the decoder to hopefully generate new believable synthetic data.\n",
    "4.   **Anomaly Detection:** Typically, the problem of anomaly detection involves classification of high dimensional data points (i.e. data with many features) into normal and anomalous samples (e.g. identifying anomolous credit card transaction). At a glance, the interpretations here can be similar to denoising, if we consider anomalies to be a type of \"noise\". There are many formulations and methematical tricks that can be used for anomaly detection. One common way is for the autoencoder to first learn a representation for the normal data. When running inference, the hope is that the reconstruction error can serve as a metric for evaluating whether a sample is anomolous or not. Intuitively, we would expect normal samples to be close in the low-dimensional space. On the contrary, anomalous samples would be expected to be far from the distribution of normal samples, yielding higher reconstruction error. Over the years many methods that leverage interpolation in the latent space, architectur improvements, and loss functions that incentivize clustering of normal and anomalous samples have been used. If you are curious to learn more about this field of research, refer to the following links: https://paperswithcode.com/task/anomaly-detection, https://github.com/yzhao062/anomaly-detection-resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aW5FPVX9-zsY",
    "tags": []
   },
   "source": [
    "### Concept Check 1.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eo5PIcjz-zsZ",
    "tags": []
   },
   "source": [
    "We will now implement the autoencoder and variational autoencoder architectures to work on the MNIST dataset (chosen for its size and ease of visualization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iolr0hrH-zsZ"
   },
   "source": [
    "## AE With Linear Layers on MNIST:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPd8CIxa-zsZ"
   },
   "source": [
    "If the following imports don't succeed, please refer to the README to ensure that you properly handled all dependencies. Note: It may take some time to run the imports the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9eg-fc41nmB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import MultivariateNormal, Normal, Independent\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqNWoHa5-zsb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reproducability\n",
    "def reset_seeds():\n",
    "    random.seed(0) # Python\n",
    "    torch.manual_seed(0) # Torch\n",
    "    np.random.seed(0) # NumPy\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Recommended to train on GPU if possible, will make training process faster\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjol3stZ-zsb"
   },
   "source": [
    "We will start with implementing a simple Vanilla Autoencoder model. We will train it on the MNIST dataset. First, let us set up the MNIST dataloaders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1bXc7GH-zsb"
   },
   "source": [
    "### Set up and visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Ux9WgHe-zsb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AE hyperparams\n",
    "epochs = 5\n",
    "lr = 1e-3\n",
    "batch_size = 64\n",
    "input_dim = 784\n",
    "latent_dim = 512\n",
    "noise = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WW8QNI3j-zsc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data\n",
    "train_dataset = datasets.MNIST(root='data/', download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='data/', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "id": "mAX2iIbL-zsc",
    "outputId": "51dc50b1-4b68-4ee9-910c-ccd52525953e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the data:\n",
    "\n",
    "# Grab first batch of images:\n",
    "images, labels = next(iter(train_dataloader))\n",
    "    \n",
    "# Print and show the first 10 samples:\n",
    "\n",
    "print(f\"Labels: {labels[0:10]}\")\n",
    "im = make_grid(images[:10], nrow=10)\n",
    "plt.figure(figsize=(10,1))\n",
    "plt.imshow(np.transpose(im.numpy(), (1, 2, 0))) #Remember that default MNIST data is CWH, but matplotlib uses WHC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FbykCBf-zsd"
   },
   "source": [
    "Now that we have set up our data, let us construct the Vanilla Autoencoder with linear layers. We know that the input to the first layer should be the flattened MNIST data dimension. Following this, the layers of the AE encoder get iteratively smaller until the bottleneck (last layer of our encoder), which dictates the latent dimension size. The decoder is a mirror reflection of the encoder. Refer to [Hinton et. al.](https://www.science.org/doi/10.1126/science.1127647) for an idea on standard architecture if you are stuck. You may also utilize the helper code MLP, but it is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiaqaNSO-zsd"
   },
   "source": [
    "### AE Model & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iH6z1atF-zsd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Some Optional Helper Code\n",
    "def mlp(in_dim, out_dim, encode=True):\n",
    "    # A helper function that compactly creates an MLP\n",
    "    # where each layer is half of (or double) the size of the previous layer\n",
    "    # depending on `encode`\n",
    "    layers = []\n",
    "    if encode:\n",
    "        next_dim = 2**math.floor(math.log2(in_dim))\n",
    "    else:\n",
    "        next_dim = 2**math.ceil(math.log2(in_dim))\n",
    "        \n",
    "    firstLayer = nn.Linear(in_dim, next_dim)\n",
    "    layers.extend([firstLayer, nn.ReLU()])\n",
    "    in_dim = next_dim\n",
    "        \n",
    "    while in_dim != out_dim:\n",
    "        if encode:\n",
    "            next_dim = in_dim // 2\n",
    "            if next_dim < out_dim:\n",
    "                next_dim = out_dim\n",
    "        else:\n",
    "            next_dim = in_dim * 2\n",
    "            if next_dim > out_dim:\n",
    "                next_dim = out_dim\n",
    "        layers.extend([nn.Linear(in_dim, next_dim), nn.ReLU()])\n",
    "        in_dim = next_dim\n",
    "        \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WwRM-Wq2WKj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, in_dim=784, latent_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def train_step(self, optimizer, x_in, x_star):\n",
    "        z = self.encode(x_in)\n",
    "        x_hat = self.decode(z)\n",
    "\n",
    "        loss = self.loss(x_star, x_hat)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, x):\n",
    "        z = self.encode(x)\n",
    "        x_hat = self.decode(z)\n",
    "        \n",
    "        loss = self.loss(x, x_hat)\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss(x, x_hat):\n",
    "        loss = None\n",
    "\n",
    "        # TODO: Mean-Squared Error Reconstruction Loss\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRPOfllw-zse"
   },
   "source": [
    "The training loop is set up for you. Feel free to tinker with the hyperparameters + model architecture at this point if you would like. On the default settings (with CUDA), this cell should take just over a minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "MVHm3k79-zsf",
    "outputId": "c9c1a54f-40c8-43fe-dbe1-5838a2aa0e1a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Training Loop for Autoencoder\n",
    "\n",
    "# model and optimizer\n",
    "reset_seeds()\n",
    "ae = AE(784, 512)\n",
    "ae.to(device)\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=lr)\n",
    "\n",
    "# train\n",
    "ae_train_losses = []\n",
    "ae_test_losses = []\n",
    "step = 0\n",
    "report_every = 500\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    # train loss:\n",
    "    for x, y in tqdm(train_dataloader):\n",
    "        x_no_noise = x.reshape(x.shape[0], -1)  # flatten\n",
    "        if noise:\n",
    "            x_in = add_noise(x_no_noise, noise_weight=0.5)\n",
    "        else:\n",
    "            x_in = x_no_noise\n",
    "        \n",
    "        x_in = x_in.to(device)\n",
    "        x_no_noise = x_no_noise.to(device)\n",
    "        loss = ae.train_step(optimizer, x_in=x_in, x_star=x_no_noise)\n",
    "        ae_train_losses.append(loss.item()) # loss every iteration\n",
    "        step += 1\n",
    "        if step % report_every == 0:\n",
    "            tqdm.write(f\"Training loss: {loss}\")\n",
    "    # ae_train_losses.append(loss.detach().numpy()) # loss after every epoch\n",
    "    # test loss\n",
    "    for b, (X_test, y_test) in enumerate(test_dataloader):\n",
    "        X_test = X_test.reshape(X_test.shape[0], -1)  # flatten\n",
    "        X_test = X_test.to(device)\n",
    "        loss = ae.test_step(X_test)\n",
    "        ae_test_losses.append(loss.item()) # loss every iteration\n",
    "    # ae_test_losses.append(loss.detach().numpy()) # loss after every epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IzLkESY-zsf"
   },
   "source": [
    "### Thinking Critically About the Architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DmRubMX-zsf"
   },
   "source": [
    "Alright, we have now trained our linear-layer (Vanilla) Autoencoder! Before directly moving on to Variational Autoencoders, let us see if we can't critically analyze the benefits and shortcomings of the Autoencoder.\n",
    "\n",
    "Let us think a little more closely about the latent representation of the Autoencoder. We note that besides optimizing the reconstruction loss, there are no further restrictions on the latent code. Because of this, we can't really ensure that the latent space representation being learned is anything semantically meaningful or even practically useful (besides being good at reconstruction of course!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOCBSRYm-zsf"
   },
   "source": [
    "Due to this lack of an informative latent space, (Vanilla) Autoencoders have more trouble with **denoising** and **content generation** tasks. One way to fix this is to regularize the latent code using some prior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRmSAaDb-zsg",
    "tags": []
   },
   "source": [
    "![VAE_diagram.jpeg](img/VAE_diagram.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">Fig 2. A diagram of the VAE architecture. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYwlgbf1-zsg"
   },
   "source": [
    "Finally, to enforce this regularization, our loss objective thus becomes:\n",
    "\n",
    "$$\\|X - g(f(X))\\|^2 + \\lambda\\mathrm{KL}[\\mathcal{N}(\\mu,\\,\\sigma^{2}), \\mathcal{N}(0,\\,I)]$$\n",
    "\n",
    "where $\\mathrm{KL}(\\cdot)$ represents the KL-divergence. Without loss of generality, we measure the KL divergence between our latent distribution and the standard normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7gJljVe-zsg"
   },
   "source": [
    "### Concept Check 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TTC0nF4-zsg"
   },
   "source": [
    "## VAE With Linear Layers on MNIST:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KF19hkFr-zsh",
    "tags": []
   },
   "source": [
    "### VAE Model & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5ZJmiH_-zsi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VAE hyperparams\n",
    "epochs = 5\n",
    "lr = 1e-3\n",
    "batch_size = 64\n",
    "noise = False\n",
    "input_dim = 784\n",
    "latent_dim=512\n",
    "regularization_weight = .0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6_o5uUr-7_D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim=1*28*28, latent_dim=512):\n",
    "        super().__init__()\n",
    "        self.z_mean = mlp(input_dim, latent_dim, encode=True)\n",
    "        self.z_log_std = mlp(input_dim, latent_dim, encode=True)\n",
    "        self.decoder = mlp(latent_dim, input_dim, encode=False)\n",
    "    \n",
    "    def _encode(self, x):\n",
    "        # Output: latent_features is output of sampling q(z|x), log_prob \n",
    "        # Hint: Check out torch.distributions, in particular Independent and Normal\n",
    "        # for calculating the log likelihood of your sampled z under q(z|x)\n",
    "        latent_features = None\n",
    "        log_prob = None\n",
    "\n",
    "        raise NotImplementedError()\n",
    "        return latent_features, log_prob \n",
    "        \n",
    "    def encode(self, x):\n",
    "        z, _ = self._encode(x)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def train_step(self, optimizer, x_in, x_star):\n",
    "        z, log_prob = self._encode(x_in)\n",
    "        x_hat = self.decode(z)\n",
    "\n",
    "        loss = self.loss(x_star, x_hat, z, log_prob)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, x):\n",
    "        z, log_prob = self._encode(x)\n",
    "        x_hat = self.decode(z)\n",
    "        \n",
    "        loss = self.loss(x, x_hat, z, log_prob)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(x, x_hat, z, log_prob, kl_weight=regularization_weight):\n",
    "        # Output: loss given true data x, reconstructed data x_hat, \n",
    "        # sampled latent variables z, log likelihood of z under q(z|x), \n",
    "        # and the weight of the KL divergence term\n",
    "        # Hint: check out torch.distributions, again specifically the MultivariateNormal\n",
    "        # class to calculate the log likelihood of latent variables under a multivariate standard normal\n",
    "        loss = None\n",
    "        raise NotImplementedError()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQhR0vmQ-zsi"
   },
   "source": [
    "Again, feel free to tinker with the architecture or the hyperparameters if you would like. With the default settings, you should expect the following cell to take ~90 seconds to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8Y5lC9M-qE3",
    "outputId": "44a63ac3-d152-4d67-b306-1de099ba6f0e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training Loop for VAE\n",
    "\n",
    "# model and optimizer\n",
    "reset_seeds()\n",
    "vae = VAE(input_dim, latent_dim)\n",
    "vae.to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "# train\n",
    "vae_train_losses = []\n",
    "vae_test_losses = []\n",
    "step = 0\n",
    "report_every = 500\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    # train loss:\n",
    "    for x, y in tqdm(train_dataloader):\n",
    "        x_no_noise = x.reshape(x.shape[0], -1)  # flatten\n",
    "        if noise:\n",
    "            x_in = add_noise(x_no_noise, noise_weight=0.5)\n",
    "        else:\n",
    "            x_in = x_no_noise\n",
    "        x_in, x_no_noise = x_in.to(device), x_no_noise.to(device)\n",
    "        loss = vae.train_step(optimizer, x_in=x_in, x_star=x_no_noise)\n",
    "        vae_train_losses.append(loss.item()) #loss every iteration\n",
    "        step += 1\n",
    "        if step % report_every == 0:\n",
    "            tqdm.write(f\"Training loss: {loss}\")\n",
    "    # vae_train_losses.append(loss.detach().numpy()) #loss after every epoch\n",
    "    # test loss\n",
    "    for b, (X_test, y_test) in enumerate(test_dataloader):\n",
    "        X_test = X_test.reshape(X_test.shape[0], -1)  # flatten\n",
    "        X_test = X_test.to(device)\n",
    "        loss = vae.test_step(X_test)\n",
    "        vae_test_losses.append(loss.item()) #loss every iteration\n",
    "    # vae_test_losses.append(loss.detach().numpy()) #loss after every epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUSN7tO--zsj"
   },
   "source": [
    "### Concept Check 2.2-2.3 (while you wait for the model to train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLKRLF-m-zsj"
   },
   "source": [
    "## Ablations and Visualizations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txXnlMsH-zsj"
   },
   "source": [
    "If you would like to understand more how these visualizations and ablations were implemented, feel free to peruse the code. However, for the most part take a close, critical look at the results. Some of these are really interesting and informative!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fqq9N8P-zsk"
   },
   "source": [
    "### 1. Loss Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "f6QU3J60-zsk",
    "outputId": "8bbab98f-6bd5-428e-b8bf-5c441b78296f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fig, ax = plt.subplots(2, figsize=(8,10))\n",
    "    ax[0].plot(ae_train_losses, label=\"AE\")\n",
    "    ax[0].plot(vae_train_losses, label=\"VAE\")\n",
    "    ax[0].set_title(\"Train Losses\")\n",
    "    ax[0].set_ylabel(\"Train Loss\")\n",
    "    ax[0].set_xlabel(\"Iteration\")\n",
    "    ax[0].legend()\n",
    "    \n",
    "    ax[1].plot(ae_test_losses, label=\"AE\")\n",
    "    ax[1].plot(vae_test_losses, label=\"VAE\")\n",
    "    ax[1].set_title(\"Test Losses\")\n",
    "    ax[1].set_ylabel(\"Test Loss\")\n",
    "    ax[1].set_xlabel(\"Iteration\")\n",
    "    ax[1].legend()\n",
    "\n",
    "#Note: if you want per epoch (or avg. epoch) losses, you will have to change the above code somewhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGoIcd6V-zsk"
   },
   "source": [
    "### 2. Visualize Reconstructed Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "CwgLxnWj-zsl",
    "outputId": "5450f122-3f5a-40fa-a21f-c537bf532b67",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the samples in bulk:\n",
    "with torch.no_grad():\n",
    "    # Grab first batch of images:\n",
    "    images, labels = next(iter(test_dataloader))\n",
    "    flattened = images.reshape((batch_size, -1))\n",
    "    flattened = flattened.to(device)\n",
    "    recon_ae = ae.decode(ae.encode(flattened)).reshape((batch_size, 1, 28, 28))\n",
    "    recon_vae = vae.decode(vae.encode(flattened)).reshape((batch_size, 1, 28, 28))\n",
    "    \n",
    "    # Print and show the first 10 samples:\n",
    "    print(f\"Labels: {labels[0:10]}\")\n",
    "    im = make_grid(images[:10], nrow=10)\n",
    "    ae_im = make_grid(recon_ae[:10], nrow=10)\n",
    "    vae_im = make_grid(recon_vae[:10], nrow=10)\n",
    "    fig, ax = plt.subplots(3, figsize=(45,4.5))\n",
    "    fig.tight_layout(pad=1.5)\n",
    "    ax[0].imshow(np.transpose(im.cpu().numpy(), (1, 2, 0))) #Remember that default MNIST data is CWH, but matplotlib uses WHC\n",
    "    ax[0].set_title(\"Original:\")\n",
    "\n",
    "    ax[1].imshow(np.transpose(ae_im.cpu().numpy(), (1, 2, 0)))\n",
    "    ax[1].set_title(\"AE Reconstruction:\")\n",
    "    \n",
    "    ax[2].imshow(np.transpose(vae_im.cpu().numpy(), (1, 2, 0)))\n",
    "    ax[2].set_title(\"VAE Reconstruction:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DurXdocO-zsm"
   },
   "source": [
    "### Concept Check 2.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YWL0do_-zsm"
   },
   "source": [
    "### 3. Visualizing the Latent Space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-RkY2Wfv-zsm",
    "outputId": "e7d595bc-cb4c-493c-9784-19302dee0823",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# analysis\n",
    "with torch.no_grad():\n",
    "    num_samples = 512\n",
    "\n",
    "    viz_dataloader = DataLoader(test_dataset, batch_size=num_samples, shuffle=True)\n",
    "    data, labels = next(iter(viz_dataloader))\n",
    "    tsne = TSNE(n_components=2, perplexity=30.0, random_state=0)\n",
    "    flattened_data = data.reshape((num_samples, -1))\n",
    "    flattened_data = flattened_data.to(device)\n",
    "    recon_ae = ae.encode(flattened_data)\n",
    "    recon_vae = vae.encode(flattened_data)\n",
    "    embedded_ae = tsne.fit_transform(recon_ae.cpu())\n",
    "    embedded_vae = tsne.fit_transform(recon_vae.cpu())\n",
    "    two_components_ae = np.hstack((embedded_ae, labels.numpy().reshape(-1, 1)))\n",
    "    two_components_ae = pd.DataFrame(data=two_components_ae)\n",
    "    two_components_ae.columns = [\"c1\", \"c2\", \"label\"]\n",
    "\n",
    "    label_groups_ae = two_components_ae.groupby(\"label\")\n",
    "    \n",
    "    two_components_vae = np.hstack((embedded_vae, labels.numpy().reshape(-1, 1)))\n",
    "    two_components_vae = pd.DataFrame(data=two_components_vae)\n",
    "    two_components_vae.columns = [\"c1\", \"c2\", \"label\"]\n",
    "\n",
    "    label_groups_vae = two_components_vae.groupby(\"label\")\n",
    "    \n",
    "    fig, ax = plt.subplots(2, figsize=(8,16))\n",
    "    fig.tight_layout(pad=1.75)\n",
    "    #plt.figure(figsize=(10, 10))\n",
    "    for label, group in label_groups_ae:\n",
    "        ax[0].scatter(group[\"c1\"], group[\"c2\"], marker=\"o\", label=label)\n",
    "\n",
    "    ax[0].legend(labels=range(10), fontsize=18)\n",
    "    ax[0].set_title(\"AE t-SNE Clustering\", fontdict={\"fontsize\": 24})\n",
    "    #ax[0].show()\n",
    "    \n",
    "    for label, group in label_groups_vae:\n",
    "        ax[1].scatter(group[\"c1\"], group[\"c2\"], marker=\"o\", label=label)\n",
    "\n",
    "    ax[1].legend(labels=range(10), fontsize=18)\n",
    "    ax[1].set_title(\"VAE t-SNE Clustering\", fontdict={\"fontsize\": 24})\n",
    "    #ax[1].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "wEQqPPRi-zso",
    "outputId": "8e94ab06-19dd-48d9-91d0-edab86d0fab1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# analysis\n",
    "with torch.no_grad():\n",
    "    num_samples = 512\n",
    "\n",
    "    viz_dataloader = DataLoader(test_dataset, batch_size=num_samples, shuffle=True)\n",
    "    data, labels = next(iter(viz_dataloader))\n",
    "\n",
    "    tsne = TSNE(n_components=3, perplexity=30.0, random_state=0)\n",
    "    flattened_data = data.reshape((num_samples, -1))\n",
    "    flattened_data = flattened_data.to(device)\n",
    "    recon_ae = ae.encode(flattened_data)\n",
    "    recon_vae = vae.encode(flattened_data)\n",
    "    embedded_ae = tsne.fit_transform(recon_ae.cpu())\n",
    "    embedded_vae = tsne.fit_transform(recon_vae.cpu())\n",
    "    two_components_ae = np.hstack((embedded_ae, labels.numpy().reshape(-1, 1)))\n",
    "    two_components_ae = pd.DataFrame(data=two_components_ae)\n",
    "    two_components_ae.columns = [\"c1\", \"c2\", \"c3\", \"label\"]\n",
    "\n",
    "    label_groups_ae = two_components_ae.groupby(\"label\")\n",
    "    \n",
    "    two_components_vae = np.hstack((embedded_vae, labels.numpy().reshape(-1, 1)))\n",
    "    two_components_vae = pd.DataFrame(data=two_components_vae)\n",
    "    two_components_vae.columns = [\"c1\", \"c2\",\"c3\", \"label\"]\n",
    "\n",
    "    label_groups_vae = two_components_vae.groupby(\"label\")\n",
    "    \n",
    "    fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "    ax0 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    fig.tight_layout(pad=1.75)\n",
    "    #plt.figure(figsize=(10, 10))\n",
    "    for label, group in label_groups_ae:\n",
    "        ax0.scatter(group[\"c1\"], group[\"c2\"], group[\"c3\"], marker=\"o\", label=label)\n",
    "\n",
    "    ax0.legend(labels=range(10), fontsize=14)\n",
    "    ax0.set_title(\"AE t-SNE Clustering\", fontdict={\"fontsize\": 18})\n",
    "    #ax[0].show()\n",
    "    \n",
    "    ax1 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    for label, group in label_groups_vae:\n",
    "        ax1.scatter(group[\"c1\"], group[\"c2\"], group[\"c3\"], marker=\"o\", label=label)\n",
    "\n",
    "    ax1.legend(labels=range(10), fontsize=14)\n",
    "    ax1.set_title(\"VAE t-SNE Clustering\", fontdict={\"fontsize\": 18})\n",
    "    #ax[1].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UE-cOxHa-zsp"
   },
   "source": [
    "### Concept Check 2.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txbEJ0sU-zsq"
   },
   "source": [
    "### 4. AE/VAE Denoising:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nw_ypt7M-zsq"
   },
   "source": [
    "Now, let us see the denoising capabilities of these architectures. Go back to the function add noise in order to add some Gaussian noise to the training data, $X$. You can adjust how reasonable the noise is by adding a coefficient in front of the noise term, which can act as a hyperparameter. Then, apply this noise to the sample when training.\n",
    "\n",
    "*(Hint: torch.randn may be helpful here. Also, consider what range of values may be outputted when you add noise. Is this consistent with the data type of MNIST? How might you account for this.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGg3PDcPqpv7"
   },
   "outputs": [],
   "source": [
    "def add_noise(tensor, mean=0., std=1., noise_weight=0.5):\n",
    "    # TODO\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFu5qu9LNOhR"
   },
   "outputs": [],
   "source": [
    "noise = True\n",
    "epochs = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVvj67lrOdze"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KKKja7CPN5XE",
    "outputId": "5c038903-c2a5-4dde-9ab2-61fc60190263"
   },
   "outputs": [],
   "source": [
    "#@title The training loops for AE and VAE are replicated below for your convenience\n",
    "#Training Loop for Autoencoder\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "#Training Loop for Autoencoder\n",
    "\n",
    "# model and optimizer\n",
    "reset_seeds()\n",
    "ae = AE(784, 512)\n",
    "ae.to(device)\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=lr)\n",
    "\n",
    "# train\n",
    "ae_train_losses = []\n",
    "ae_test_losses = []\n",
    "step = 0\n",
    "report_every = 500\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    # train loss:\n",
    "    for x, y in tqdm(train_dataloader):\n",
    "        x_no_noise = x.reshape(x.shape[0], -1)  # flatten\n",
    "        if noise:\n",
    "            x_in = add_noise(x_no_noise, noise_weight=0.5)\n",
    "        else:\n",
    "            x_in = x_no_noise\n",
    "        \n",
    "        x_in = x_in.to(device)\n",
    "        x_no_noise = x_no_noise.to(device)\n",
    "        loss = ae.train_step(optimizer, x_in=x_in, x_star=x_no_noise)\n",
    "        ae_train_losses.append(loss.item()) # loss every iteration\n",
    "        step += 1\n",
    "        if step % report_every == 0:\n",
    "            tqdm.write(f\"Training loss: {loss}\")\n",
    "    # ae_train_losses.append(loss.detach().numpy()) # loss after every epoch\n",
    "    # test loss\n",
    "    for b, (X_test, y_test) in enumerate(test_dataloader):\n",
    "        X_test = X_test.reshape(X_test.shape[0], -1)  # flatten\n",
    "        X_test = X_test.to(device)\n",
    "        loss = ae.test_step(X_test)\n",
    "        ae_test_losses.append(loss.item()) # loss every iteration\n",
    "    # ae_test_losses.append(loss.detach().numpy()) # loss after every epoch\n",
    "\n",
    "# Training Loop for VAE\n",
    "\n",
    "# model and optimizer\n",
    "reset_seeds()\n",
    "vae = VAE(input_dim, latent_dim)\n",
    "vae.to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "# train\n",
    "vae_train_losses = []\n",
    "vae_test_losses = []\n",
    "step = 0\n",
    "report_every = 500\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    # train loss:\n",
    "    for x, y in tqdm(train_dataloader):\n",
    "        x_no_noise = x.reshape(x.shape[0], -1)  # flatten\n",
    "        if noise:\n",
    "            x_in = add_noise(x_no_noise, noise_weight=0.5)\n",
    "        else:\n",
    "            x_in = x_no_noise\n",
    "        x_in, x_no_noise = x_in.to(device), x_no_noise.to(device)\n",
    "        loss = vae.train_step(optimizer, x_in=x_in, x_star=x_no_noise)\n",
    "        vae_train_losses.append(loss.item()) #loss every iteration\n",
    "        step += 1\n",
    "        if step % report_every == 0:\n",
    "            tqdm.write(f\"Training loss: {loss}\")\n",
    "    # vae_train_losses.append(loss.detach().numpy()) #loss after every epoch\n",
    "    # test loss\n",
    "    for b, (X_test, y_test) in enumerate(test_dataloader):\n",
    "        X_test = X_test.reshape(X_test.shape[0], -1)  # flatten\n",
    "        X_test = X_test.to(device)\n",
    "        loss = vae.test_step(X_test)\n",
    "        vae_test_losses.append(loss.item()) #loss every iteration\n",
    "    # vae_test_losses.append(loss.detach().numpy()) #loss after every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "ONH8AJlg-zsr",
    "outputId": "6c58b790-28dd-4035-b89f-2930d0d45887",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fig, ax = plt.subplots(2, figsize=(8,10))\n",
    "    ax[0].plot(ae_train_losses, label=\"AE\")\n",
    "    ax[0].plot(vae_train_losses, label=\"VAE\")\n",
    "    ax[0].set_title(\"Train Losses\")\n",
    "    ax[0].set_ylabel(\"Train Loss\")\n",
    "    ax[0].set_xlabel(\"Iteration\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(ae_test_losses, label=\"AE\")\n",
    "    ax[1].plot(vae_test_losses, label=\"VAE\")\n",
    "    ax[1].set_title(\"Test Losses\")\n",
    "    ax[1].set_ylabel(\"Test Loss\")\n",
    "    ax[1].set_xlabel(\"Iteration\")\n",
    "    ax[1].legend()\n",
    "\n",
    "#Note: if you want per epoch (or avg. epoch) losses, you will have to change the above code somewhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 664
    },
    "id": "fHN86yYr-zsr",
    "outputId": "a72ee124-d489-42c4-a1fd-b53b0da5e4ed",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the samples in bulk:\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Grab first batch of images:\n",
    "    images, labels = next(iter(test_dataloader))\n",
    "\n",
    "\n",
    "    # Add Noise to images:\n",
    "    noise_images = add_noise(images, noise_weight=0.65)\n",
    "    flattened_noise_images = noise_images.reshape((batch_size, -1))\n",
    "    flattened_noise_images = flattened_noise_images.to(device)\n",
    "    recon_ae = ae.decode(ae.encode(flattened_noise_images)).reshape((batch_size, 1, 28, 28))\n",
    "    recon_vae = vae.decode(vae.encode(flattened_noise_images)).reshape((batch_size, 1, 28, 28))\n",
    "    \n",
    "    # Print and show the first 10 samples:\n",
    "    print(f\"Labels: {labels[0:10]}\")\n",
    "    im = make_grid(images[:10], nrow=10)\n",
    "    noise_im = make_grid(noise_images[:10], nrow=10)\n",
    "    ae_im = make_grid(recon_ae[:10], nrow=10)\n",
    "    vae_im = make_grid(recon_vae[:10], nrow=10)\n",
    "    fig, ax = plt.subplots(4, figsize=(60,6))\n",
    "    fig.tight_layout(pad=1.5)\n",
    "    ax[0].imshow(np.transpose(im.cpu().numpy(), (1, 2, 0))) #Remember that default MNIST data is CWH, but matplotlib uses WHC\n",
    "    ax[0].set_title(\"Original:\")\n",
    "    \n",
    "    ax[1].imshow(np.transpose(noise_im.cpu().numpy(), (1, 2, 0)))\n",
    "    ax[1].set_title(\"With added noise:\")\n",
    "\n",
    "    ax[2].imshow(np.transpose(ae_im.cpu().numpy(), (1, 2, 0)))\n",
    "    ax[2].set_title(\"AE Reconstruction:\")\n",
    "    \n",
    "    ax[3].imshow(np.transpose(vae_im.cpu().numpy(), (1, 2, 0)))\n",
    "    ax[3].set_title(\"VAE Reconstruction:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJJWkMSU-zst",
    "tags": []
   },
   "source": [
    "### Concept Check 2.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kCrIT4d7-zst",
    "outputId": "088f5b58-4cf9-47b5-aede-fc5669d990d5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# analysis\n",
    "with torch.no_grad():\n",
    "    num_samples = 512\n",
    "\n",
    "    viz_dataloader = DataLoader(test_dataset, batch_size=num_samples, shuffle=True)\n",
    "    data, labels = next(iter(viz_dataloader))\n",
    "    flattened_data = data.reshape((num_samples, -1))\n",
    "    flattened_data = flattened_data.to(device)\n",
    "\n",
    "    tsne = TSNE(n_components=2, perplexity=30.0, random_state=0)\n",
    "    recon_ae = ae.encode(flattened_data)\n",
    "    recon_vae = vae.encode(flattened_data)\n",
    "    embedded_ae = tsne.fit_transform(recon_ae.cpu())\n",
    "    embedded_vae = tsne.fit_transform(recon_vae.cpu())\n",
    "    two_components_ae = np.hstack((embedded_ae, labels.numpy().reshape(-1, 1)))\n",
    "    two_components_ae = pd.DataFrame(data=two_components_ae)\n",
    "    two_components_ae.columns = [\"c1\", \"c2\", \"label\"]\n",
    "\n",
    "    label_groups_ae = two_components_ae.groupby(\"label\")\n",
    "    \n",
    "    two_components_vae = np.hstack((embedded_vae, labels.numpy().reshape(-1, 1)))\n",
    "    two_components_vae = pd.DataFrame(data=two_components_vae)\n",
    "    two_components_vae.columns = [\"c1\", \"c2\", \"label\"]\n",
    "\n",
    "    label_groups_vae = two_components_vae.groupby(\"label\")\n",
    "    \n",
    "    fig, ax = plt.subplots(2, figsize=(8,16))\n",
    "    fig.tight_layout(pad=1.75)\n",
    "    #plt.figure(figsize=(10, 10))\n",
    "    for label, group in label_groups_ae:\n",
    "        ax[0].scatter(group[\"c1\"], group[\"c2\"], marker=\"o\", label=label)\n",
    "\n",
    "    ax[0].legend(labels=range(10), fontsize=18)\n",
    "    ax[0].set_title(\"AE t-SNE Clustering\", fontdict={\"fontsize\": 24})\n",
    "    #ax[0].show()\n",
    "    \n",
    "    for label, group in label_groups_vae:\n",
    "        ax[1].scatter(group[\"c1\"], group[\"c2\"], marker=\"o\", label=label)\n",
    "\n",
    "    ax[1].legend(labels=range(10), fontsize=18)\n",
    "    ax[1].set_title(\"VAE t-SNE Clustering\", fontdict={\"fontsize\": 24})\n",
    "    #ax[1].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "id": "iZ7gtlip-zst",
    "outputId": "5186521e-d23f-46b1-8f19-b31283c6f93b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# analysis\n",
    "with torch.no_grad():\n",
    "    num_samples = 512\n",
    "\n",
    "    viz_dataloader = DataLoader(test_dataset, batch_size=num_samples, shuffle=True)\n",
    "    data, labels = next(iter(viz_dataloader))\n",
    "\n",
    "    flattened_data = data.reshape((num_samples, -1)).to(device)\n",
    "    tsne = TSNE(n_components=3, perplexity=30.0, random_state=0)\n",
    "    recon_ae = ae.encode(flattened_data)\n",
    "    recon_vae = vae.encode(flattened_data)\n",
    "    embedded_ae = tsne.fit_transform(recon_ae.cpu())\n",
    "    embedded_vae = tsne.fit_transform(recon_vae.cpu())\n",
    "    two_components_ae = np.hstack((embedded_ae, labels.numpy().reshape(-1, 1)))\n",
    "    two_components_ae = pd.DataFrame(data=two_components_ae)\n",
    "    two_components_ae.columns = [\"c1\", \"c2\", \"c3\", \"label\"]\n",
    "\n",
    "    label_groups_ae = two_components_ae.groupby(\"label\")\n",
    "    \n",
    "    two_components_vae = np.hstack((embedded_vae, labels.numpy().reshape(-1, 1)))\n",
    "    two_components_vae = pd.DataFrame(data=two_components_vae)\n",
    "    two_components_vae.columns = [\"c1\", \"c2\",\"c3\", \"label\"]\n",
    "\n",
    "    label_groups_vae = two_components_vae.groupby(\"label\")\n",
    "    \n",
    "    fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "    ax0 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    fig.tight_layout(pad=1.75)\n",
    "    #plt.figure(figsize=(10, 10))\n",
    "    for label, group in label_groups_ae:\n",
    "        ax0.scatter(group[\"c1\"], group[\"c2\"], group[\"c3\"], marker=\"o\", label=label)\n",
    "\n",
    "    ax0.legend(labels=range(10), fontsize=14)\n",
    "    ax0.set_title(\"AE t-SNE Clustering\", fontdict={\"fontsize\": 18})\n",
    "    #ax[0].show()\n",
    "    \n",
    "    ax1 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    for label, group in label_groups_vae:\n",
    "        ax1.scatter(group[\"c1\"], group[\"c2\"], group[\"c3\"], marker=\"o\", label=label)\n",
    "\n",
    "    ax1.legend(labels=range(10), fontsize=14)\n",
    "    ax1.set_title(\"VAE t-SNE Clustering\", fontdict={\"fontsize\": 18})\n",
    "    #ax[1].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9RGAh8S-zsu",
    "tags": []
   },
   "source": [
    "### 5. Generating New Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Za44Xvf1-zsu",
    "tags": []
   },
   "source": [
    "#### 5a. AE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "7Yu6Abtj-zsu",
    "outputId": "a0dfda03-2c10-4aaf-ea4c-b78c5c9d751c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ae.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # calculate mean and std of latent code, generated takining in test images as inputs \n",
    "    images, labels = next(iter(test_dataloader))\n",
    "  \n",
    "    flattened_images = images.reshape((batch_size, -1)).to(device)\n",
    "    latent = vae.encode(flattened_images)\n",
    "    latent = latent.cpu()\n",
    "    mean = latent.mean(dim=0)\n",
    "    std = (latent - mean).pow(2).mean(dim=0).sqrt()\n",
    "\n",
    "    # sample latent vectors from the normal distribution\n",
    "    latent = torch.randn(batch_size, latent_dim)*std + mean\n",
    "\n",
    "    # reconstruct images from the random latent vectors\n",
    "    latent = latent.to(device)\n",
    "    img_recon = ae.decode(latent)\n",
    "    img_recon = img_recon.reshape((batch_size, 1, 28, 28)).cpu()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 8.5))\n",
    "    im = make_grid(img_recon[:20], nrow=10)\n",
    "    plt.imshow(np.transpose(im.numpy(), (1, 2, 0)))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qryxq-Lc-zsu",
    "tags": []
   },
   "source": [
    "#### 5b. VAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "69orDIrg-zsv",
    "outputId": "5cf957d6-5302-4fa4-a5d4-18c04c52c756",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # calculate mean and std of latent code, generated takining in test images as inputs \n",
    "    images, labels = next(iter(test_dataloader))\n",
    "    images = images.to(device)\n",
    "    latent = vae.encode(images.reshape((batch_size, -1)))\n",
    "    latent = latent.cpu()\n",
    "    mean = latent.mean(dim=0)\n",
    "    std = (latent - mean).pow(2).mean(dim=0).sqrt()\n",
    "\n",
    "    # sample latent vectors from the normal distribution\n",
    "    latent = torch.randn(batch_size, latent_dim)*std + mean\n",
    "\n",
    "    # reconstruct images from the random latent vectors\n",
    "    latent = latent.to(device)\n",
    "    img_recon = vae.decode(latent)\n",
    "    img_recon = img_recon.reshape((batch_size, 1, 28, 28)).cpu()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 8.5))\n",
    "    im = make_grid(img_recon[:20], nrow=10)\n",
    "    plt.imshow(np.transpose(im.numpy(), (1, 2, 0)))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7B4U3EA-zsv"
   },
   "source": [
    "### Concept Check 2.3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYQZM8pk-zsv"
   },
   "source": [
    "## References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqrt8SmV-zsv"
   },
   "source": [
    "Any references utilized in this project can be found in the README of our repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bzxm13wPTiE8",
    "outputId": "92cfd355-664a-48a7-c559-f509cee3ddb9"
   },
   "outputs": [],
   "source": [
    "#@title A faster version (no training losses, only denoising, AE)\n",
    "#Training Loop for Autoencoder\n",
    "\n",
    "# model and optimizer\n",
    "reset_seeds()\n",
    "ae = AE(784, 512)\n",
    "ae.to(device)\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=lr, capturable=True)\n",
    "\n",
    "warmup_iterator = iter(train_dataloader)\n",
    "x, y = next(warmup_iterator)\n",
    "x_no_noise = x.reshape(x.shape[0], -1)\n",
    "x_in = add_noise(x_no_noise, noise_weight=0.5)\n",
    "static_input = torch.zeros_like(x_in, device=device).copy_(x_in)\n",
    "static_target = torch.zeros_like(x_no_noise, device=device).copy_(x_no_noise)\n",
    "\n",
    "s = torch.cuda.Stream()\n",
    "s.wait_stream(torch.cuda.current_stream())\n",
    "with torch.cuda.stream(s):\n",
    "    for i in range(3):\n",
    "        x, _ = next(warmup_iterator)\n",
    "        x_no_noise = x.reshape(x.shape[0], -1)\n",
    "        x_in = add_noise(x_no_noise, noise_weight=0.5)\n",
    "        x_in = x_in.to(device)\n",
    "        x_no_noise = x_no_noise.to(device)\n",
    "        loss = ae.train_step(optimizer, x_in=x_in, x_star=x_no_noise)\n",
    "torch.cuda.current_stream().wait_stream(s)\n",
    "\n",
    "# capture\n",
    "g = torch.cuda.CUDAGraph()\n",
    "# Sets grads to None before capture, so backward() will create\n",
    "# .grad attributes with allocations from the graph's private pool\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "with torch.cuda.graph(g):\n",
    "    static_input = add_noise(static_target, noise_weight=0.5)\n",
    "    loss = ae.train_step(optimizer, x_in=static_input, x_star=static_target)\n",
    "\n",
    "# train\n",
    "step = 0\n",
    "report_every = 500\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    # train loss:\n",
    "    for x, y in tqdm(train_dataloader):\n",
    "        x_no_noise = x.reshape(x.shape[0], -1)  # flatten\n",
    "        if x.shape != static_input.shape:\n",
    "          continue\n",
    "        static_target.copy_(x_no_noise)\n",
    "\n",
    "        g.replay()\n",
    "        \n",
    "        step += 1\n",
    "        if step % report_every == 0:\n",
    "            tqdm.write(f\"Training loss: {loss}\")\n",
    "    # ae_train_losses.append(loss.detach().numpy()) # loss after every epoch\n",
    "    # test loss\n",
    "    for b, (X_test, y_test) in enumerate(test_dataloader):\n",
    "        X_test = X_test.reshape(X_test.shape[0], -1)  # flatten\n",
    "        X_test = X_test.to(device)\n",
    "        loss = ae.test_step(X_test)\n",
    "        ae_test_losses.append(loss.item()) # loss every iteration\n",
    "    # ae_test_losses.append(loss.detach().numpy()) # loss after every epoch"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
