{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GIzgyg50w5G"
   },
   "source": [
    "# Purpose and Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEnKmCF2rEUL"
   },
   "source": [
    "You have probably just arrived from getting a thorough understanding of the architectures and capabilities of Autoencoders and Variational Autoencoders! Now, we will look at something more state-of-the-art, the **VQ-VAE** model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERczBpsOrEUL"
   },
   "source": [
    "After completing this notebook, you will:\n",
    "1. Have a thorough understanding of the architecture and capabilites of the VQ-VAE\n",
    "2. Be able to implement the VQ-VAE to reconstruct CIFAR-10 images\n",
    "3. Evaluate the models' performance and explore a plethora of visualizations and ablations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPHlZCnhrEUM"
   },
   "source": [
    "## Motivation for VQ-VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlxXDYrNrEUM"
   },
   "source": [
    "Recall, that we motivated the VAE architecture by saying that we would like to regularize the latent space of the AE. We modeled the latent space as being sampled from a Gaussian distribution. This is a convenient and useful interpretation, but can we do better? Yes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plWa0nYirEUM"
   },
   "source": [
    "![coco_instance_segmentation.jpeg](img/coco_instance_segmentation.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZhxSigJrEUM"
   },
   "source": [
    "<p style=\"text-align: center;\">Fig 1. An image taken from the COCO dataset </p>\n",
    "\n",
    "[Source](https://manipulation.csail.mit.edu/segmentation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENUURKRYrEUM"
   },
   "source": [
    "The above image, taken from the COCO dataset, illustrates a very important facet of much of the data that is of interest to the deep learning community. Whether it is language, scene images, or audio files, many semantically meaningful data have *discrete elements within them.*\n",
    "\n",
    "Thus, a logical new improvement to our VAE bottleneck would be to **sample from a discrete distribution instead of a continuous one**. That is precisely the main motivation for the VQ-VAE artchitecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8K200yb8rEUM"
   },
   "source": [
    "### Concept Check 3-3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frBWvzf004DO"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.distributions import MultivariateNormal, Normal, Independent\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 42
    },
    "id": "PYTm3IZp7eO0",
    "outputId": "f77bed65-4c63-4528-98a8-149a6e74fc00"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHcpmq39tqau"
   },
   "outputs": [],
   "source": [
    "#Reproducability Checks:\n",
    "random.seed(0) #Python\n",
    "torch.manual_seed(0) #Torch\n",
    "np.random.seed(0) #NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpFB23_MrEUO"
   },
   "source": [
    "## Implementing VQ-VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KctYR_JWPb5"
   },
   "source": [
    "### Residual Layers For Image Processing\n",
    "We use ResNet blocks for image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMNeeU8218It"
   },
   "outputs": [],
   "source": [
    "class ResidualLayerBlock(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, res_h_dim):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=in_dim, out_channels = res_h_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(res_h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=res_h_dim, out_channels=h_dim, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(h_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.block(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0udDzv_S30qM"
   },
   "outputs": [],
   "source": [
    "class ResidualLayers(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, res_h_dim, n_res_layers):\n",
    "        super().__init__()\n",
    "        self.n_res_layers = n_res_layers\n",
    "        self.layers = nn.ModuleList(\n",
    "            [ResidualLayerBlock(in_dim, h_dim, res_h_dim)] * n_res_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return F.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aniKnNNyWaKE"
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpP7l0Nc49I7"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, res_h_dim, n_res_layers):\n",
    "        super().__init__()\n",
    "        kernel = 4\n",
    "        stride = 2\n",
    "        #Maybe remove batch norms?\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, h_dim // 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(h_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(h_dim // 2, h_dim, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(h_dim, h_dim, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(h_dim),\n",
    "            ResidualLayers(h_dim, h_dim, res_h_dim, n_res_layers)\n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "      return self.conv_block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i8tFO_1rEUP"
   },
   "source": [
    "## [Important] Vector Quantizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPDSL9TSbXwi"
   },
   "source": [
    "Recall that loss is defined as:\n",
    "\n",
    "$$Recon(x, \\hat{x}) + ||sg(z_e(x)) - e||_2^2 + \\beta ||z_e(x) - e||_2^2$$\n",
    "\n",
    "Where the first term is our codebook loss to keep our codebook close to our encoder outputs, and the third term is keep our codebook committed to a discrete distribution. Note that we are going to be quantizing over each individual channel pixel, so we we sample a channel from the discrete distribution. If the original image size is say for example $[64, 3, 32, 32]$, we first rearrange it to have each element be a channel pixel, i.e $[64, 32, 32, 3]$, then flatten it to get each individuala channel yielding $[64 * 32 * 32, 3]$. Now we have $64 * 32 * 32$ seperate channel pixels to query from the latent distribution, and map them to its closest latent distribution value. It should be noted that the outputs from the gradient wont have any actual gradients, because of the nearest neighbor sampling. To deal with this, we copy the gradient from the outputs to the inputs. In other words, we do:\n",
    "\n",
    "$$e := z_e(x) + sg(e - z_e(x))$$\n",
    "\n",
    "This allows the gradients of $e$ to be copied over to $z_e(x)$ and backprop to our encoder. In this problem, you will code the the vector quantitization modules to sample the latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-6kXfmTrEUP"
   },
   "source": [
    "### Concept Check 3.2.1-3.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tak01lnW8LnJ"
   },
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        \n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "        \n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n",
    "        self._commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # convert inputs from BCHW -> BHWC\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)\n",
    "        \n",
    "        # Calculate distances and quantize\n",
    "        #=================YOUR CODE HERE=======================\n",
    "        #sample from the discrete latent distribution using nearest\n",
    "        #neighbor sampling.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #=================END CODE HERE=========================\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate loss for loss function\n",
    "        #=================YOUR CODE HERE========================\n",
    "        e_latent_loss = ?\n",
    "        q_latent_loss = ?\n",
    "        loss = ?\n",
    "        #=================END CODE HERE=========================\n",
    "        \n",
    "\n",
    "\n",
    "        #Allow for output gradients to be copied over to input during backprop\n",
    "        #=================YOUR CODE HERE=======================\n",
    "        quantized = ?\n",
    "        #=================ENC CODE HERE========================\n",
    "\n",
    "\n",
    "        \n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "\n",
    "        \n",
    "        # convert quantized from BHWC -> BCHW\n",
    "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DczK3oFwrEUQ"
   },
   "source": [
    "## The Decoder Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ypFbXbOC0vu"
   },
   "source": [
    "Uses the sampled discrete distribution and reconstruct the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ry4h3vBLCihl"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, n_res_layers, res_h_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        kernel = 4\n",
    "        stride = 2\n",
    "\n",
    "        self.inverse_conv_stack = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_dim, h_dim, kernel_size=kernel-1, stride=stride-1, padding=1),\n",
    "            ResidualLayers(h_dim, h_dim, res_h_dim, n_res_layers),\n",
    "            nn.ConvTranspose2d(h_dim, h_dim // 2,\n",
    "                               kernel_size=kernel, stride=stride, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(h_dim//2, 3, kernel_size=kernel,\n",
    "                               stride=stride, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.inverse_conv_stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiaoVIaArEUQ"
   },
   "source": [
    "## The VQAE Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cz0zHXPPDGLv"
   },
   "source": [
    "This is the main module for the VQ-VAE. The model consists of \n",
    "\n",
    "$$Encoder \\to VectorQuantitization \\to Decoder \\to \\hat{x}$$\n",
    "\n",
    "Each image will have its own discrete mapping. Let $f_{vq}(x)$ be the discrete mapping from an image $x$ to its discrete mapping. Once the model is trained, we will have good mappings for $x \\to f_{vq}(x)$. However, to actually sample from the VQ-VAE, we need to learn patterns from the learned $f_{vq}(x)$. Once we can sample $f_{vq}$ perhaps using PixelCNN, we can send it into the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uz6Pv4qLB6iO"
   },
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, h_dim, res_h_dim, n_res_layers,\n",
    "                 n_embeddings, embedding_dim, beta, save_img_embedding_map=False):\n",
    "        super(VQVAE, self).__init__()\n",
    "        # encode image into continuous latent space\n",
    "        self.encoder = Encoder(3, h_dim, n_res_layers, res_h_dim)\n",
    "        self.pre_quantization_conv = nn.Conv2d(\n",
    "            h_dim, embedding_dim, kernel_size=1, stride=1)\n",
    "        # pass continuous latent vector through discretization bottleneck\n",
    "        self.vector_quantization = VectorQuantizer(\n",
    "            n_embeddings, embedding_dim, beta)\n",
    "        # decode the discrete latent representation\n",
    "        self.decoder = Decoder(embedding_dim, h_dim, n_res_layers, res_h_dim)\n",
    "\n",
    "        if save_img_embedding_map:\n",
    "            self.img_to_embedding_map = {i: [] for i in range(n_embeddings)}\n",
    "        else:\n",
    "            self.img_to_embedding_map = None\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        #=================YOUR CODE HERE====================\n",
    "        #code the forward function of the VQVAE. Compute the\n",
    "        #reconstruction x_hat, the embedding_loss, and the perplexity\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        #================END YOUR CODE HERE==================\n",
    "        return embedding_loss, x_hat, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bsT6nrBzO5R"
   },
   "source": [
    "**Downloading and normalizing the CIFAR 10 Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FF75B3AvC2fO",
    "outputId": "10902a49-008c-4469-8a68-8c3d390879a9"
   },
   "outputs": [],
   "source": [
    "training_data = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
    "                                  ]))\n",
    "\n",
    "validation_data = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
    "                                  ]))\n",
    "data_variance = np.var(training_data.data / 255.0)\n",
    "data_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_cR-9VXvzNn"
   },
   "source": [
    "## Define our Hyperparameters \n",
    "You should be able to get a validation loss of 0.35. The solution hyperparameters gets a validation loss of 0.1 in 6 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KI3huZC2E2Bb"
   },
   "outputs": [],
   "source": [
    "#======================YOUR CODE HERE=======================\n",
    "batch_size = 32\n",
    "n_hiddens = 64\n",
    "n_residual_hiddens = 32\n",
    "n_residual_layers = 2\n",
    "embedding_dim = 64\n",
    "n_embeddings = 1024\n",
    "beta = .1\n",
    "lr = 1e-5\n",
    "epochs = 10\n",
    "#=====================END CODE HERE======================\n",
    "noise=False\n",
    "noise_weight=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3k4plQSanzDO"
   },
   "outputs": [],
   "source": [
    "def add_noise(tensor, mean=0., std=1., noise_weight=0.5):\n",
    "    #Copy the add_noise function from the ae_vae notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94awn_j6hbuL"
   },
   "outputs": [],
   "source": [
    "vqvae = VQVAE(n_hiddens, n_residual_hiddens, n_residual_layers,\n",
    "              n_embeddings, embedding_dim, \n",
    "              beta).to(device)\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size = batch_size, shuffle = True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data,batch_size= batch_size,shuffle=True)\n",
    "optimizer = torch.optim.Adam(vqvae.parameters(), lr=lr, amsgrad=False)\n",
    "\n",
    "train_res_recon_error = []\n",
    "test_res_recon_error = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xc61mXPKI3yJ",
    "outputId": "4faf8446-ac52-4242-c710-42334441f6db"
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        vqvae.train()\n",
    "        for data, target in tepoch:\n",
    "            data_no_noise = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if noise:\n",
    "                data = add_noise(data_no_noise, noise_weight=noise_weight)\n",
    "            else:\n",
    "                data = data_no_noise\n",
    "            vq_loss, data_recon, perplexity = vqvae(data_no_noise)\n",
    "            recon_error = F.mse_loss(data_recon, data_no_noise) / data_variance\n",
    "            loss = recon_error + vq_loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            tepoch.set_postfix(loss=float(loss.detach().cpu()))\n",
    "            train_res_recon_error.append(recon_error.item())\n",
    "\n",
    "    avg_loss = 0\n",
    "    vqvae.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in validation_loader:\n",
    "            data = data.to(device)\n",
    "\n",
    "            vq_loss, data_recon, perplexity = vqvae(data)\n",
    "            recon_error = F.mse_loss(data_recon, data) / data_variance\n",
    "            loss = recon_error.item() * batch_size\n",
    "\n",
    "            avg_loss += loss / len(validation_data)\n",
    "            test_res_recon_error.append(loss)\n",
    "    \n",
    "    print(f'Validation Loss: {avg_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtOVhItkI_s7"
   },
   "source": [
    "## Preliminary VQ-VAE Loss Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "fTIFzi1F69-P",
    "outputId": "275901af-1bfb-4599-a600-ebca1d88df8f"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_res_recon_error)\n",
    "plt.xlabel('Batches')\n",
    "plt.ylabel('Training Reconstruction Error')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(test_res_recon_error)\n",
    "plt.xlabel('batches')\n",
    "plt.ylabel('Test Reconstruction Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2mWiTlh1Gio"
   },
   "source": [
    "## Preliminary VQ-VAE Reconstruction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "XeVk9CcrKLhz",
    "outputId": "e0b1c3ae-1074-4ae2-b68e-d6445cc288d8"
   },
   "outputs": [],
   "source": [
    "vqvae.eval()\n",
    "temp_loader = torch.utils.data.DataLoader(validation_data,batch_size=16,shuffle=True)\n",
    "(valid_originals, _) = next(iter(temp_loader))\n",
    "valid_originals = valid_originals.to(device)\n",
    "\n",
    "_, valid_recon, _ = vqvae(valid_originals)\n",
    "def show(img, title):\n",
    "    npimg = img.numpy()\n",
    "    fig = plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "    fig.axes.set_title(title)\n",
    "\n",
    "\n",
    "show(torchvision.utils.make_grid(valid_originals.cpu())+0.5, \"Original\")\n",
    "plt.show()\n",
    "show(torchvision.utils.make_grid(valid_recon.cpu().data) + 0.5, \"VQ-VAE Reconstructed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSj5p_My600t"
   },
   "source": [
    "##Sampling Uniformly From Latent Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "CL5p7gOMrtIe",
    "outputId": "d336f6a3-d1af-478b-b032-c9ba470aec8c"
   },
   "outputs": [],
   "source": [
    "def sample_model(model):\n",
    "    #sample 8 x 8 embedding vectors\n",
    "    encoding_indices = torch.argmin(torch.rand(size = [8 * 8, n_embeddings]), dim=1).to(device).unsqueeze(1)\n",
    "    encodings = torch.zeros(encoding_indices.shape[0], n_embeddings, device=device)\n",
    "    encodings.scatter_(1, encoding_indices, 1)\n",
    "    quantized = torch.matmul(encodings, model.vector_quantization._embedding.weight).view(1, 8, 8, 64)\n",
    "    quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
    "    z_e = model.decoder(quantized)\n",
    "    return z_e\n",
    "    \n",
    "plt.imshow(sample_model(vqvae).squeeze(0).permute(1, 2, 0).cpu().detach() + 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-ZpJG2OUl5y"
   },
   "source": [
    "### Concept Check 3.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqYdj12VrOwB"
   },
   "source": [
    "## Ablations & Visualizations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PTk-7uFuhxh"
   },
   "source": [
    "### Quick AE Implementation Using Same Encoder-Decoder Architecture as VQ-VAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKVNE0PVwSZv"
   },
   "outputs": [],
   "source": [
    "# AE/VAE hyperparams\n",
    "batch_size = 64\n",
    "n_hiddens = 64\n",
    "n_residual_hiddens = 32\n",
    "n_residual_layers = 2\n",
    "embedding_dim = 64\n",
    "code_size=8\n",
    "\n",
    "epochs = 6\n",
    "lr = 1e-3\n",
    "noise=False\n",
    "lin_dim=256\n",
    "regularization_weight = .0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIIL5Tl7ufve"
   },
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, h_dim, res_h_dim, n_res_layers, embedding_dim, lin_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h_dim = h_dim\n",
    "        # encode image into continuous latent space\n",
    "        self.encoder = Encoder(3, h_dim, n_res_layers, res_h_dim)\n",
    "\n",
    "        #FC Projections\n",
    "        self.fc1 = nn.Linear(h_dim*code_size*code_size, lin_dim)\n",
    "        self.fc2 = nn.Linear(lin_dim, h_dim*code_size*code_size)\n",
    "\n",
    "        # decode the discrete latent representation\n",
    "        self.decoder = Decoder(embedding_dim, h_dim, n_res_layers, res_h_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(-1, self.h_dim*code_size*code_size)\n",
    "        return self.fc1(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc2(z)\n",
    "        z = z.view(-1, self.h_dim, code_size, code_size)\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def train_step(self, optimizer, x_in, x_star):\n",
    "        z = self.encode(x_in)\n",
    "        x_hat = self.decode(z)\n",
    "\n",
    "        loss = self.loss(x_star, x_hat)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, x):\n",
    "        z = self.encode(x)\n",
    "        x_hat = self.decode(z)\n",
    "        \n",
    "        loss = self.loss(x, x_hat)\n",
    "        return loss\n",
    "    @staticmethod\n",
    "    def loss(x, x_hat):\n",
    "        #Mean-Squared Error Reconstruction Loss\n",
    "        criterion = nn.MSELoss()\n",
    "        return criterion(x, x_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TSJ33rhcvBXD",
    "outputId": "05a7c4f1-ac59-4891-a7a6-4a5500f5aba9"
   },
   "outputs": [],
   "source": [
    "ae = AE(n_hiddens, n_residual_hiddens, n_residual_layers, embedding_dim, lin_dim).to(device)\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=lr)\n",
    "# train\n",
    "ae_train_losses = []\n",
    "ae_test_losses = []\n",
    "step = 0\n",
    "report_every = 500\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    #train loss:\n",
    "    for x, y in tqdm(train_loader):\n",
    "        x_no_noise = x.to(device)\n",
    "        if noise:\n",
    "            x_in = add_noise(x_no_noise, noise_weight=0.5)\n",
    "        else:\n",
    "            x_in = x_no_noise\n",
    "        loss = ae.train_step(optimizer, x_in=x_in, x_star=x_no_noise)\n",
    "        ae_train_losses.append(loss.cpu().detach().numpy()) #loss every iteration\n",
    "        step += 1\n",
    "        if step % report_every == 0:\n",
    "            print(f\"Training loss: {loss}\")\n",
    "    #ae_train_losses.append(loss.detach().numpy()) #loss after every epoch\n",
    "    #test loss\n",
    "    ae.eval()\n",
    "    with torch.no_grad():\n",
    "      for X_test, y_test in validation_loader:\n",
    "          X_test = X_test.to(device)\n",
    "          loss = ae.test_step(X_test)\n",
    "          ae_test_losses.append(loss.cpu().detach().numpy()) #loss every iteration\n",
    "      #ae_test_losses.append(loss.detach().numpy()) #loss after every epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghRxNmUh1q3V"
   },
   "source": [
    "### Quick VAE Implementation Using Same Encoder-Decoder Architecture as VQ-VAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdCxn2fK1txe"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, h_dim, res_h_dim, n_res_layers, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.z_mean = Encoder(3, h_dim, n_res_layers, res_h_dim)\n",
    "        self.z_log_std = Encoder(3, h_dim, n_res_layers, res_h_dim)\n",
    "        self.decoder = Decoder(embedding_dim, h_dim, n_res_layers, res_h_dim)\n",
    "\n",
    "        self.h_dim = h_dim\n",
    "        #FC Projections\n",
    "        self.fc1 = nn.Linear(h_dim*code_size*code_size, lin_dim)\n",
    "        self.fc2 = nn.Linear(lin_dim, h_dim*code_size*code_size)\n",
    "    \n",
    "    def _encode(self, x):\n",
    "        x_mean = self.z_mean(x)\n",
    "        x_flat = x_mean.view(-1, self.h_dim*code_size*code_size)\n",
    "        z_mean = self.fc1(x_flat)\n",
    "        \n",
    "\n",
    "        x_std = self.z_log_std(x)\n",
    "        x_flat = x_std.view(-1, self.h_dim*code_size*code_size)\n",
    "        z_log_std = self.fc1(x_flat)\n",
    "        z_log_std = nn.Sigmoid()(z_log_std)\n",
    "        # reparameterization trick\n",
    "        z_std = torch.exp(z_log_std)\n",
    "\n",
    "        eps = torch.randn_like(z_std)\n",
    "        z = z_mean + eps * z_std\n",
    "        # log prob\n",
    "        # 'd' not sampled on purpose\n",
    "        # to show reparameterization trick\n",
    "        d = Independent(Normal(z_mean, z_std), 1)\n",
    "        log_prob = d.log_prob(z)\n",
    "        \n",
    "        return z_mean + eps * z_std, log_prob\n",
    "    \n",
    "    def encode(self, x):\n",
    "        z, _ = self._encode(x)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc2(z)\n",
    "        z = z.view(-1, self.h_dim, code_size, code_size)\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def train_step(self, optimizer, x_in, x_star):\n",
    "        z, log_prob = self._encode(x_in)\n",
    "        x_hat = self.decode(z)\n",
    "        loss = self.loss(x_star, x_hat, z, log_prob)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, x):\n",
    "        z, log_prob = self._encode(x)\n",
    "        x_hat = self.decode(z)\n",
    "        \n",
    "        loss = self.loss(x, x_hat, z, log_prob)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(x, x_hat, z, log_prob, kl_weight=regularization_weight):\n",
    "        criterion = nn.MSELoss()\n",
    "        reconst_loss = criterion(x, x_hat)\n",
    "\n",
    "        z_dim = z.shape[-1]\n",
    "        standard_normal = MultivariateNormal(torch.zeros(z_dim).to(device), \n",
    "                                             torch.eye(z_dim).to(device))\n",
    "        #print(MultivariateNormal.device)\n",
    "        kld_loss = (log_prob - standard_normal.log_prob(z)).mean()\n",
    "        \n",
    "        return reconst_loss + kl_weight * kld_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UNOM9n2F1t9Y",
    "outputId": "bf82cbe7-1f2b-460c-d95e-80a3c72095fc"
   },
   "outputs": [],
   "source": [
    "vae = VAE(n_hiddens, n_residual_hiddens, n_residual_layers, embedding_dim).to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "# train\n",
    "vae_train_losses = []\n",
    "vae_test_losses = []\n",
    "step = 0\n",
    "report_every = 500\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    #train loss:\n",
    "    for x, y in tqdm(train_loader):\n",
    "        x_no_noise = x.to(device)\n",
    "        if noise:\n",
    "            x_in = add_noise(x_no_noise, noise_weight=0.5)\n",
    "        else:\n",
    "            x_in = x_no_noise\n",
    "        loss = vae.train_step(optimizer, x_in=x_in, x_star=x_no_noise)\n",
    "        vae_train_losses.append(loss.cpu().detach().numpy()) #loss every iteration\n",
    "        step += 1\n",
    "        if step % report_every == 0:\n",
    "            print(f\"Training loss: {loss}\")\n",
    "    #vae_train_losses.append(loss.detach().numpy()) #loss after every epoch\n",
    "    #test loss\n",
    "    for b, (X_test, y_test) in enumerate(validation_loader):\n",
    "        X_test = X_test.to(device)\n",
    "        loss = vae.test_step(X_test)\n",
    "        vae_test_losses.append(loss.cpu().detach().numpy()) #loss every iteration\n",
    "    #vae_test_losses.append(loss.detach().numpy()) #loss after every epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dg3ZiwIe4ojg"
   },
   "source": [
    "### 1. Loss Visualization (AE vs. VAE vs. VQVAE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "VS1f1u1kREOL",
    "outputId": "2ed52535-9fc8-41c5-85f7-98da67950106"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fig, ax = plt.subplots(2, figsize=(8,10))\n",
    "    ax[0].plot(ae_train_losses, label=\"AE\")\n",
    "    ax[0].plot(vae_train_losses, label=\"VAE\")\n",
    "    ax[0].plot(train_res_recon_error, label=\"VQ-VAE\")\n",
    "    ax[0].set_title(\"Train Losses\")\n",
    "    ax[0].set_ylabel(\"Train Loss\")\n",
    "    ax[0].set_xlabel(\"Iteration\")\n",
    "    ax[0].legend()\n",
    "    \n",
    "    ax[1].plot(ae_test_losses, label=\"AE\")\n",
    "    ax[1].plot(vae_test_losses, label=\"VAE\")\n",
    "    ax[1].plot(test_res_recon_error, label=\"VQ-VAE\")\n",
    "    ax[1].set_title(\"Test Losses\")\n",
    "    ax[1].set_ylabel(\"Test Loss\")\n",
    "    ax[1].set_xlabel(\"Iteration\")\n",
    "    ax[1].legend()\n",
    "\n",
    "#Note: if you want per epoch (or avg. epoch) losses, you will have to change the above code somewhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTofI5sqki7C"
   },
   "source": [
    "### Concept Check 3.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxF9Z6bjrEUS"
   },
   "source": [
    "### 2. Reconstruction Visualization (AE vs. VAE vs. VQVAE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 736
    },
    "id": "MJp6wseCRnqj",
    "outputId": "629ba629-7b77-4fea-f1ee-8c07d282b385"
   },
   "outputs": [],
   "source": [
    "#Visualize the samples in bulk:\n",
    "ae.eval()\n",
    "vae.eval()\n",
    "vqvae.eval()\n",
    "with torch.no_grad():\n",
    "    #Grab first batch of images:\n",
    "    for images, labels in validation_loader:\n",
    "        recon_ae = ae.decode(ae.encode(images.to(device)))\n",
    "        recon_vae = vae.decode(vae.encode(images.to(device)))\n",
    "        _, recon_vqvae, _ = vqvae(images.to(device))\n",
    "        break\n",
    "\n",
    "\n",
    "    #Print and show the first 10 samples:\n",
    "\n",
    "    print(f\"Labels: {labels[0:10]}\")\n",
    "    mean=[0.5,0.5,0.5]\n",
    "    std=[1.0,1.0,1.0]\n",
    "    inv_normalize = transforms.Normalize(\n",
    "        mean= [-m/s for m, s in zip(mean, std)],\n",
    "        std= [1/s for s in std]\n",
    "    )\n",
    "    im = make_grid(inv_normalize(images[0:10])*255, nrow=10)\n",
    "    ae_im = make_grid(inv_normalize(recon_ae[:10])*255, nrow=10)\n",
    "    vae_im = make_grid(inv_normalize(recon_vae[:10])*255, nrow=10)\n",
    "    vqvae_im = make_grid(inv_normalize(recon_vqvae[:10])*255, nrow=10)\n",
    "\n",
    "    fig, ax = plt.subplots(4, figsize=(80,8))\n",
    "    fig.tight_layout(pad=1.5)\n",
    "    ax[0].imshow(np.transpose(im.numpy(), (1, 2, 0)).astype('uint8')) #Remember that default MNIST data is CWH, but matplotlib uses WHC\n",
    "    ax[0].set_title(\"Original:\")\n",
    "\n",
    "    ax[1].imshow(np.transpose(ae_im.cpu().numpy(), (1, 2, 0)).astype('uint8'))\n",
    "    ax[1].set_title(\"AE Reconstruction:\")\n",
    "    \n",
    "    ax[2].imshow(np.transpose(vae_im.cpu().numpy(), (1, 2, 0)).astype('uint8'))\n",
    "    ax[2].set_title(\"VAE Reconstruction:\")\n",
    "\n",
    "    ax[3].imshow(np.transpose(vqvae_im.cpu().numpy(), (1, 2, 0)).astype('uint8'))\n",
    "    ax[3].set_title(\"VQ-VAE Reconstruction:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Zn-qyqyko6i"
   },
   "source": [
    "### Concept Check 3.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQt62NQtrEUS"
   },
   "source": [
    "### 3. Denoising Ablation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lP5F8fxCTTmg"
   },
   "source": [
    "Now, let us see the denoising capabilities of these architectures. Go back to the function add_noise in order to add some Gaussian noise to the training data, $X$. You can just copy-paste this from the AE-VAE notebook and **set the noise hyperparameter to True**. Then, apply this noise to the sample when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "TBwLby0sTvmR",
    "outputId": "f27e35e9-5f2a-4a6d-adb0-5789ea040d12"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fig, ax = plt.subplots(2, figsize=(8,10))\n",
    "    ax[0].plot(ae_train_losses, label=\"AE\")\n",
    "    ax[0].plot(vae_train_losses, label=\"VAE\")\n",
    "    ax[0].plot(train_res_recon_error, label=\"VQ-VAE\")\n",
    "    ax[0].set_title(\"Train Losses\")\n",
    "    ax[0].set_ylabel(\"Train Loss\")\n",
    "    ax[0].set_xlabel(\"Iteration\")\n",
    "    ax[0].legend()\n",
    "    \n",
    "    ax[1].plot(ae_test_losses, label=\"AE\")\n",
    "    ax[1].plot(vae_test_losses, label=\"VAE\")\n",
    "    ax[1].plot(test_res_recon_error, label=\"VQ-VAE\")\n",
    "    ax[1].set_title(\"Test Losses\")\n",
    "    ax[1].set_ylabel(\"Test Loss\")\n",
    "    ax[1].set_xlabel(\"Iteration\")\n",
    "    ax[1].legend()\n",
    "\n",
    "#Note: if you want per epoch (or avg. epoch) losses, you will have to change the above code somewhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "7w6rOnxHTv_-",
    "outputId": "f4aee268-8676-4e6c-93a1-a09f0a8a7c14"
   },
   "outputs": [],
   "source": [
    "#Visualize the samples in bulk:\n",
    "ae.eval()\n",
    "vae.eval()\n",
    "vqvae.eval()\n",
    "with torch.no_grad():\n",
    "    #Grab first batch of images:\n",
    "    for images, labels in validation_loader:\n",
    "        recon_ae = ae.decode(ae.encode(images.to(device)))\n",
    "        recon_vae = vae.decode(vae.encode(images.to(device)))\n",
    "        _, recon_vqvae, _ = vqvae(images.to(device))\n",
    "        break\n",
    "\n",
    "\n",
    "    #Print and show the first 10 samples:\n",
    "\n",
    "    print(f\"Labels: {labels[0:10]}\")\n",
    "    mean=[0.5,0.5,0.5]\n",
    "    std=[1.0,1.0,1.0]\n",
    "    inv_normalize = transforms.Normalize(\n",
    "        mean= [-m/s for m, s in zip(mean, std)],\n",
    "        std= [1/s for s in std]\n",
    "    )\n",
    "    im = make_grid(inv_normalize(images[0:10])*255, nrow=10)\n",
    "    ae_im = make_grid(inv_normalize(recon_ae[:10])*255, nrow=10)\n",
    "    vae_im = make_grid(inv_normalize(recon_vae[:10])*255, nrow=10)\n",
    "    vqvae_im = make_grid(inv_normalize(recon_vqvae[:10])*255, nrow=10)\n",
    "\n",
    "    fig, ax = plt.subplots(4, figsize=(80,8))\n",
    "    fig.tight_layout(pad=1.5)\n",
    "    ax[0].imshow(np.transpose(im.numpy(), (1, 2, 0)).astype('uint8')) #Remember that default MNIST data is CWH, but matplotlib uses WHC\n",
    "    ax[0].set_title(\"Original:\")\n",
    "\n",
    "    ax[1].imshow(np.transpose(ae_im.cpu().numpy(), (1, 2, 0)).astype('uint8'))\n",
    "    ax[1].set_title(\"AE Reconstruction:\")\n",
    "    \n",
    "    ax[2].imshow(np.transpose(vae_im.cpu().numpy(), (1, 2, 0)).astype('uint8'))\n",
    "    ax[2].set_title(\"VAE Reconstruction:\")\n",
    "\n",
    "    ax[3].imshow(np.transpose(vqvae_im.cpu().numpy(), (1, 2, 0)).astype('uint8'))\n",
    "    ax[3].set_title(\"VQ-VAE Reconstruction:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7djQtPlrEUS"
   },
   "source": [
    "### 4. Sample Generation Ablation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "om1ui9Q0A7R_"
   },
   "source": [
    "**Sampling**\n",
    "\n",
    "Unlike VAEs, VQ-VAEs can be difficult to sample from. Recall in VAEs, we enforced $p(z) \\sim N(0, I)$ using a KL divergence loss. This makes it easy to sample a VAE. But VQ-VAEs don't have this nice property. \n",
    "\n",
    "To see this, we can try sampling $z$ uniformly, and see the resulting image. To be more explicit, for each latent pixel in the 8x8 space, we sample one of the 512 codebook vectors uniformly, then feed the whole thing into the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFRBdcsLA7R_"
   },
   "outputs": [],
   "source": [
    "# reusing the code from earlier\n",
    "plt.imshow(sample_model(vqvae).squeeze(0).permute(1, 2, 0).cpu().detach() + 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ja_NuwajA7R_"
   },
   "source": [
    "Chances are the generated image is difficult to recognize. We have 512 codebook vectors, each of which can go into 1 of the squares in the 8 by 8 embedding. So despite being a discrete representation, there are some $64^{512} \\approx 10^{900}$ possible things that can be encoded. So it makes sense that sampling uniformly in this space likely won't reveal anything meaningful. \n",
    "\n",
    "Run the code below to see an example of the embedding index per latent pixel. Do this a couple times for different images. What do you see, are there patterns you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vs2mOuRxA7R_"
   },
   "outputs": [],
   "source": [
    "# vqvae latents\n",
    "viz_loader = torch.utils.data.DataLoader(training_data, batch_size = 1, shuffle = True)\n",
    "X, _ = next(iter(viz_loader))\n",
    "X = X.to(device)\n",
    "z_e = vqvae.encoder(X)\n",
    "z_e = vqvae.pre_quantization_conv(z_e)\n",
    "_, z_q, _, encodings = vqvae.vector_quantization(z_e)\n",
    "plt.imshow(encodings.cpu().numpy(), cmap='gray', aspect='auto')\n",
    "plt.xlabel('embedding index')\n",
    "plt.ylabel('latent pixel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2MEW_jcA7R_"
   },
   "source": [
    "**Train an autoregressive model**\n",
    "\n",
    "From the visualizations above, it should be relatively clear that the embeddings chosen per pixel are neither uniformly distributed, nor independent. We'd been breaking apart $p(z_1,\\ldots, z_n) = \\prod_{i=1}^n p(z_i)$. What would make the generation better would be to learn a conditional distribution of each latent pixel based on the previous ones: $p(z_k | z_{k-1} \\ldots z_1)$. In the original VQ-VAE paper, this is done using an autoregressive model, PixelCNN. Here, we attempt to do the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tz2UV4wJA7R_"
   },
   "outputs": [],
   "source": [
    "# PixelCNNs aren't the focus of this project.\n",
    "# This was taken from: https://github.com/jzbontar/pixelcnn-pytorch/\n",
    "\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super(MaskedConv2d, self).__init__(*args, **kwargs)\n",
    "        assert mask_type in {'A', 'B'}\n",
    "        self.register_buffer('mask', self.weight.data.clone())\n",
    "        _, _, kH, kW = self.weight.size()\n",
    "        self.mask.fill_(1)\n",
    "        self.mask[:, :, kH // 2, kW // 2 + (mask_type == 'B'):] = 0\n",
    "        self.mask[:, :, kH // 2 + 1:] = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv2d, self).forward(x)\n",
    "\n",
    "fm = 64\n",
    "pixCNN = nn.Sequential(\n",
    "    MaskedConv2d('A', 512,  fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    nn.Conv2d(fm, 512, 1)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VOeEGbKA7R_"
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "optimizer = torch.optim.Adam(pixCNN.parameters(), lr=lr)\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_losses = []\n",
    "for _ in range(epochs):\n",
    "    for X, _ in tqdm(train_loader):\n",
    "        if X.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        # get our embedding\n",
    "        X = X.to(device)\n",
    "        z_e = vqvae.encoder(X)\n",
    "        z_e = vqvae.pre_quantization_conv(z_e)\n",
    "        _, z_q, _, encodings = vqvae.vector_quantization(z_e) \n",
    "\n",
    "        enc_inds = encodings.argmax(dim=1, keepdims=True)\n",
    "        encodings_img = encodings.reshape(batch_size, 8, 8, -1)\n",
    "        encodings_img = encodings_img.permute(0, 3, 1, 2).float().to(device)\n",
    "\n",
    "        # learn\n",
    "        optimizer.zero_grad()\n",
    "        output = pixCNN(encodings_img)\n",
    "\n",
    "        output = output.permute(0, 2, 3, 1)\n",
    "        output = output.reshape(-1, 512)\n",
    "\n",
    "        loss = F.cross_entropy(output, enc_inds.squeeze(1).long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "    print(train_losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iogPWJUA7R_"
   },
   "outputs": [],
   "source": [
    "torch.save(pixCNN.state_dict(), 'PixCNN.pt')\n",
    "pixCNN.load_state_dict(torch.load('PixCNN.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5rkl9nlA7R_"
   },
   "outputs": [],
   "source": [
    "sample = torch.zeros(1, 512, 8, 8).to(device)\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        output = pixCNN(sample)\n",
    "        weights = F.softmax(output[:, :, i, j])\n",
    "        embed_indx = torch.multinomial(weights, 1).item()\n",
    "        sample[:, embed_indx, i, j] = 1\n",
    "\n",
    "sample = sample.squeeze(0).reshape(512, 64)\n",
    "sample = sample.permute((1, 0))\n",
    "\n",
    "quantized = torch.matmul(sample, vqvae.vector_quantization._embedding.weight)\n",
    "quantized = quantized.view(1, 8, 8, 64)\n",
    "quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
    "z_e = vqvae.decoder(quantized)\n",
    "\n",
    "plt.imshow(z_e.squeeze(0).permute(1, 2, 0).cpu().detach() + 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kBMVEB87-tJ"
   },
   "source": [
    "## **[Important]** Bringing it All Together:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRBpIpGA8HKi"
   },
   "source": [
    "From your exploration of the different ablations, you should now be able to trace the developmental ideas that inform autoencoder architectures. We started with the AE, which is a method for dimensionality reduction that is motivated by the simple idea that the latent representation of some data will probably be informative if it can be reconstructed to yield back the original input data. However, there were some clear limitations. (1) The latent space wasn't organized in any meaningful way. (2) A lack of any regularization of the latent code prevents AE from doing better in denoising. (3) The first two problems prevent the AE from being effective in sample generation. \n",
    "\n",
    "\n",
    "These issues are fixed in the VAE by modeling the latent space as being sampled from a Gaussian. We usually find worse reconstruction thanks to this regularization, but we gain richer capabilities in denoising and sample generation.\n",
    "\n",
    "\n",
    "The final improvement we explored comes from the VQ-VAE, which replaces the continuous Gaussian in the VAE with a discrete distribution of latent codes. This is helpful (1) for preventing posterior collapse, but more importantly (2), most data types are a composition of *discrete elements*. Thus, we find the VQ-VAE is an improvement over the VAE in nearly all ways.\n",
    "\n",
    "This is why in the world of deep learning, the term *better* is so ambiguous and flexible. You may have noticed that the AE provided the best reconstruction in the ablations that only tested reconstruction. If we think critically about the loss functions and architectures of the 3 models, this makes sense. The AE is *only* motivated to improve reconstruction while the other models have regularization to ensure we **learn more useful representations** (ones which can be used for denoising and sample generation, while the AE's cannot). In some ways, the analogy of \"AE is to VAE/VQ-VAE as least squares is to ridge regression\" is apt here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Lhn7aRaF2Pr"
   },
   "source": [
    "## Where to Go Next:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyf3LW0tF6Su"
   },
   "source": [
    "Congratulations! You have finished all our demos, ablations, and conceptual questions on AEs, VAEs, and VQ-VAEs. This is a very solid foundation for utilizing autoencoder models and the subfield of representation learning in general. From here on, nearly all research areas and model architectures will be somewhat based on those that you've seen. Here are some ideas for what to explore next:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbE9axOLyIcw"
   },
   "source": [
    "\n",
    "1.   [CVAE](https://papers.nips.cc/paper_files/paper/2015/hash/8d55a249e6baa5c06772297520da2051-Abstract.html): Conditional VAEs are the next step following VAEs. The formulation allows for a more deterministic method for generating samples.\n",
    "2.   [RQVAE](http://arxiv.org/abs/2203.01941): The next stage of progress in extending the capabilities of the VQ-VAE.\n",
    "3.   [MISA](http://arxiv.org/abs/2005.03545): A fascinating use of AE architectures for multi-modal sentiment analysis.\n",
    "4.   [GANs](http://arxiv.org/abs/1406.2661): Generative Adversarial Networks (GANs) are somewhat similar in motivation to AE architectures. At a very high-level, it's much like an \"inverted\" AE that has the sole purpose of data generation and data discrimination.\n",
    "5.   Anomaly Detection: AE and GAN-like architectures being used in this subfield of research is very common.\n",
    "  - https://doi.org/10.1186/s42400-022-00134-9\n",
    "  - http://arxiv.org/abs/1511.05644\n",
    "  - http://arxiv.org/abs/2003.10713\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDtYitRNUAX9"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y73MCRnzUGxF"
   },
   "source": [
    "Any references utilized in this project can be found in the README of our repo"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
