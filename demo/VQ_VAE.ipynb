{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GIzgyg50w5G"
      },
      "source": [
        "# Purpose and Objectives:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEnKmCF2rEUL"
      },
      "source": [
        "You have probably just arrived from getting a thorough understanding of the architectures and capabilities of Autoencoders and Variational Autoencoders! Now, we will look at something more state-of-the-art, the **VQ-VAE** model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERczBpsOrEUL"
      },
      "source": [
        "After completing this notebook, you will:\n",
        "1. Have a thorough understanding of the architecture and capabilites of the VQ-VAE\n",
        "2. Be able to implement the VQ-VAE to reconstruct CIFAR-10 images\n",
        "3. Evaluate the models' performance and explore a plethora of visualizations and ablations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPHlZCnhrEUM"
      },
      "source": [
        "## Motivation for VQ-VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlxXDYrNrEUM"
      },
      "source": [
        "Recall, that we motivated the VAE architecture by saying that we would like to regularize the latent space of the AE. We modeled the latent space as being sampled from a Gaussian distribution. This is a convenient and useful interpretation, but can we do better? Yes!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plWa0nYirEUM"
      },
      "source": [
        "![coco_instance_segmentation.jpeg](attachment:4bad17a6-b078-4a21-977d-4f53fd91eacc.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZhxSigJrEUM"
      },
      "source": [
        "<p style=\"text-align: center;\">Fig 1. An image taken from the COCO dataset </p>\n",
        "\n",
        "[Source](https://manipulation.csail.mit.edu/segmentation.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENUURKRYrEUM"
      },
      "source": [
        "The above image, taken from the COCO dataset, illustrates a very important facet of much of the data that is of interest to the deep learning community. Whether it is language, scene images, or audio files, many semantically meaningful data have *discrete elements within them.*\n",
        "\n",
        "Thus, a logical new improvement to our VAE bottleneck would be to **sample from a discrete distribution instead of a continuous one**. That is precisely the main motivation for the VQ-VAE artchitecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K200yb8rEUM"
      },
      "source": [
        "### Concept Check 3-3.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "frBWvzf004DO"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.distributions import MultivariateNormal, Normal, Independent\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PYTm3IZp7eO0",
        "outputId": "c58f0590-25e3-4f81-e38b-7ed361879833"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jHcpmq39tqau"
      },
      "outputs": [],
      "source": [
        "#Reproducability Checks:\n",
        "random.seed(0) #Python\n",
        "torch.manual_seed(0) #Torch\n",
        "np.random.seed(0) #NumPy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpFB23_MrEUO"
      },
      "source": [
        "## Implementing VQ-VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KctYR_JWPb5"
      },
      "source": [
        "### Residual Layers For Image Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uMNeeU8218It"
      },
      "outputs": [],
      "source": [
        "class ResidualLayerBlock(nn.Module):\n",
        "    def __init__(self, in_dim, h_dim, res_h_dim):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=in_dim, out_channels = res_h_dim, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(res_h_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=res_h_dim, out_channels=h_dim, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(h_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.block(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0udDzv_S30qM"
      },
      "outputs": [],
      "source": [
        "class ResidualLayers(nn.Module):\n",
        "    def __init__(self, in_dim, h_dim, res_h_dim, n_res_layers):\n",
        "        super().__init__()\n",
        "        self.n_res_layers = n_res_layers\n",
        "        self.layers = nn.ModuleList(\n",
        "            [ResidualLayerBlock(in_dim, h_dim, res_h_dim)] * n_res_layers\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return F.relu(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aniKnNNyWaKE"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kpP7l0Nc49I7"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_dim, h_dim, res_h_dim, n_res_layers):\n",
        "        super().__init__()\n",
        "        kernel = 4\n",
        "        stride = 2\n",
        "        #Maybe remove batch norms?\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(in_dim, h_dim // 2, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(h_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(h_dim // 2, h_dim, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(h_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(h_dim, h_dim, kernel_size = 3, stride = 1, padding = 1),\n",
        "            nn.BatchNorm2d(h_dim),\n",
        "            ResidualLayers(h_dim, h_dim, res_h_dim, n_res_layers)\n",
        "        ) \n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv_block(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i8tFO_1rEUP"
      },
      "source": [
        "## [Important] Vector Quantizer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPDSL9TSbXwi"
      },
      "source": [
        "Recall that loss is defined as:\n",
        "\n",
        "$$Recon(x, \\hat{x}) + ||sg(z_e(x)) - e||_2^2 + \\beta ||z_e(x) - e||_2^2$$\n",
        "\n",
        "Where the first term is our codebook loss to keep our codebook close to our encoder outputs, and the third term is keep our codebook committed to a discrete distribution. Note that we are going to be quantizing over each individual channel pixel, so we we sample a channel from the discrete distribution. If the original image size is say for example $[64, 3, 32, 32]$, we first rearrange it to have each element be a channel pixel, i.e $[64, 32, 32, 3]$, then flatten it to get each individuala channel yielding $[64 * 32 * 32, 3]$. Now we have $64 * 32 * 32$ seperate channel pixels to query from the latent distribution, and map them to its closest latent distribution value. It should be noted that the outputs from the gradient wont have any actual gradients, because of the nearest neighbor sampling. To deal with this, we copy the gradient from the outputs to the inputs. In other words, we do:\n",
        "\n",
        "$$e := z_e(x) + sg(e - z_e(x))$$\n",
        "\n",
        "This allows the gradients of $e$ to be copied over to $z_e(x)$ and backprop to our encoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-6kXfmTrEUP"
      },
      "source": [
        "### Concept Check 3.2.1-3.2.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Tak01lnW8LnJ"
      },
      "outputs": [],
      "source": [
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "        \n",
        "        self._embedding_dim = embedding_dim\n",
        "        self._num_embeddings = num_embeddings\n",
        "        \n",
        "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
        "        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n",
        "        self._commitment_cost = commitment_cost\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # convert inputs from BCHW -> BHWC\n",
        "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "        input_shape = inputs.shape\n",
        "        \n",
        "        # Flatten input\n",
        "        flat_input = inputs.view(-1, self._embedding_dim)\n",
        "        \n",
        "        # Calculate distances\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
        "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
        "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
        "            \n",
        "        # Encoding\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1) \n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "        \n",
        "        # Quantize and unflatten\n",
        "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
        "        \n",
        "        # Loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
        "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
        "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
        "        \n",
        "        quantized = inputs + (quantized - inputs).detach() #allows for copied gradients\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "        \n",
        "        # convert quantized from BHWC -> BCHW\n",
        "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DczK3oFwrEUQ"
      },
      "source": [
        "## The Decoder Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ypFbXbOC0vu"
      },
      "source": [
        "Uses the sampled discrete distribution and reconstruct the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Ry4h3vBLCihl"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_dim, h_dim, n_res_layers, res_h_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        kernel = 4\n",
        "        stride = 2\n",
        "\n",
        "        self.inverse_conv_stack = nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_dim, h_dim, kernel_size=kernel-1, stride=stride-1, padding=1),\n",
        "            ResidualLayers(h_dim, h_dim, res_h_dim, n_res_layers),\n",
        "            nn.ConvTranspose2d(h_dim, h_dim // 2,\n",
        "                               kernel_size=kernel, stride=stride, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(h_dim//2, 3, kernel_size=kernel,\n",
        "                               stride=stride, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.inverse_conv_stack(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiaoVIaArEUQ"
      },
      "source": [
        "## The VQAE Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz0zHXPPDGLv"
      },
      "source": [
        "This is the main module for the VQ-VAE. The model consists of \n",
        "\n",
        "$$Encoder \\to VectorQuantitization \\to Decoder \\to \\hat{x}$$\n",
        "\n",
        "Each image will have its own discrete mapping. Let $f_{vq}(x)$ be the discrete mapping from an image $x$ to its discrete mapping. Once the model is trained, we will have good mappings for $x \\to f_{vq}(x)$. However, to actually sample from the VQ-VAE, we need to learn patterns from the learned $f_{vq}(x)$. Once we can sample $f_{vq}$ perhaps using PixelCNN, we can send it into the decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uz6Pv4qLB6iO"
      },
      "outputs": [],
      "source": [
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, h_dim, res_h_dim, n_res_layers,\n",
        "                 n_embeddings, embedding_dim, beta, save_img_embedding_map=False):\n",
        "        super(VQVAE, self).__init__()\n",
        "        # encode image into continuous latent space\n",
        "        self.encoder = Encoder(3, h_dim, n_res_layers, res_h_dim)\n",
        "        self.pre_quantization_conv = nn.Conv2d(\n",
        "            h_dim, embedding_dim, kernel_size=1, stride=1)\n",
        "        # pass continuous latent vector through discretization bottleneck\n",
        "        self.vector_quantization = VectorQuantizer(\n",
        "            n_embeddings, embedding_dim, beta)\n",
        "        # decode the discrete latent representation\n",
        "        self.decoder = Decoder(embedding_dim, h_dim, n_res_layers, res_h_dim)\n",
        "\n",
        "        if save_img_embedding_map:\n",
        "            self.img_to_embedding_map = {i: [] for i in range(n_embeddings)}\n",
        "        else:\n",
        "            self.img_to_embedding_map = None\n",
        "\n",
        "    def forward(self, x, verbose=False):\n",
        "\n",
        "        z_e = self.encoder(x)\n",
        "\n",
        "        z_e = self.pre_quantization_conv(z_e)\n",
        "        embedding_loss, z_q, perplexity, _ = self.vector_quantization(z_e)\n",
        "        x_hat = self.decoder(z_q)\n",
        "\n",
        "        return embedding_loss, x_hat, perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF75B3AvC2fO",
        "outputId": "8522b593-3356-4629-b2e3-5c4d2b9fe26c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "training_data = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True,\n",
        "                                  transform=transforms.Compose([\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
        "                                  ]))\n",
        "\n",
        "validation_data = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True,\n",
        "                                  transform=transforms.Compose([\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
        "                                  ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGegHjM5JynC",
        "outputId": "5e3dd53a-7979-43d3-8ed0-5241bd2f5373"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06328692405746414"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "data_variance = np.var(training_data.data / 255.0)\n",
        "data_variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_cR-9VXvzNn"
      },
      "source": [
        "## Define our Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KI3huZC2E2Bb"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "n_hiddens = 64\n",
        "n_residual_hiddens = 32\n",
        "n_residual_layers = 2\n",
        "embedding_dim = 64\n",
        "n_embeddings = 512\n",
        "beta = .25\n",
        "lr = 3e-4\n",
        "epochs = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_NXpZKa-DN6j"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(training_data, batch_size = batch_size, shuffle = True)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_data,batch_size=32,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cnjFgzeoFicw"
      },
      "outputs": [],
      "source": [
        "vqvae = VQVAE(n_hiddens, n_residual_hiddens, n_residual_layers,\n",
        "              n_embeddings, embedding_dim, \n",
        "              beta).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecVbBpFsBae9",
        "outputId": "28b2a8ac-0051-463b-b1aa-10aac41ffc66"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VQVAE(\n",
              "  (encoder): Encoder(\n",
              "    (conv_block): Sequential(\n",
              "      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "      (3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU()\n",
              "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (8): ResidualLayers(\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x ResidualLayerBlock(\n",
              "            (block): Sequential(\n",
              "              (0): ReLU()\n",
              "              (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "              (2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (3): ReLU()\n",
              "              (4): Conv2d(2, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_quantization_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (vector_quantization): VectorQuantizer(\n",
              "    (_embedding): Embedding(512, 64)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (inverse_conv_stack): Sequential(\n",
              "      (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ResidualLayers(\n",
              "        (layers): ModuleList(\n",
              "          (0-1): 2 x ResidualLayerBlock(\n",
              "            (block): Sequential(\n",
              "              (0): ReLU()\n",
              "              (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "              (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              (3): ReLU()\n",
              "              (4): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "      (3): ReLU()\n",
              "      (4): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "vqvae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JD3SCabaH39I"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(vqvae.parameters(), lr=lr, amsgrad=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "Xc61mXPKI3yJ",
        "outputId": "0ea1b4b8-80e9-4c5f-957b-ab2b6388fa5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [00:46<00:00, 16.75batch/s, loss=4.15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.9354950910568228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [00:43<00:00, 18.00batch/s, loss=1.86]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.7877601999282833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|██████▊   | 536/782 [00:29<00:13, 18.05batch/s, loss=0.783]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-b3ded3b2bc34>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mrecon_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdata_variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecon_error\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvq_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_res_recon_error = []\n",
        "test_res_recon_error = []\n",
        "train_res_perplexity = []\n",
        "for epoch in range(epochs):\n",
        "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
        "        vqvae.train()\n",
        "        for data, target in tepoch:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            vq_loss, data_recon, perplexity = vqvae(data)\n",
        "            recon_error = F.mse_loss(data_recon, data) / data_variance\n",
        "            loss = recon_error + vq_loss\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            tepoch.set_postfix(loss=float(loss.detach().cpu()))\n",
        "            train_res_recon_error.append(recon_error.item())\n",
        "            train_res_perplexity.append(perplexity.item())\n",
        "\n",
        "    avg_loss = 0\n",
        "    vqvae.eval()\n",
        "    for data, target in validation_loader:\n",
        "        data = data.to(device)\n",
        "\n",
        "        vq_loss, data_recon, perplexity = vqvae(data)\n",
        "        recon_error = F.mse_loss(data_recon, data) / data_variance\n",
        "        loss = recon_error.item() * 32\n",
        "\n",
        "        avg_loss += loss / len(validation_data)\n",
        "        test_res_recon_error.append(recon_error.item())\n",
        "    \n",
        "    print(f'Validation Loss: {avg_loss}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dNEaanL1Bz62"
      },
      "outputs": [],
      "source": [
        "# Michael, I don't think we need this anymore since we are able to clearly train the VQVAE on Colab\n",
        "torch.save(vqvae.state_dict(), 'VQ-VAE_Model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTIFzi1F69-P"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_res_recon_error)\n",
        "plt.xlabel('Batches')\n",
        "plt.ylabel('Training Reconstruction Error')\n",
        "plt.show()\n",
        "#plt.xlabel('Batches')\n",
        "#plt.ylabel('Training Perplexity')\n",
        "#plt.plot(perplexity)\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeVk9CcrKLhz"
      },
      "outputs": [],
      "source": [
        "vqvae.eval()\n",
        "validation_loader = torch.utils.data.DataLoader(validation_data,batch_size=16,shuffle=True)\n",
        "(valid_originals, _) = next(iter(validation_loader))\n",
        "valid_originals = valid_originals.to(device)\n",
        "\n",
        "_, valid_recon, _ = vqvae(valid_originals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on9HLcMaUHOg"
      },
      "outputs": [],
      "source": [
        "def show(img):\n",
        "    npimg = img.numpy()\n",
        "    fig = plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
        "    fig.axes.get_xaxis().set_visible(False)\n",
        "    fig.axes.get_yaxis().set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zyz7j86kUOiP"
      },
      "outputs": [],
      "source": [
        "show(torchvision.utils.make_grid(valid_recon.cpu().data) + 0.5, )\n",
        "plt.show()\n",
        "show(torchvision.utils.make_grid(valid_originals.cpu())+0.5, )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL5p7gOMrtIe"
      },
      "outputs": [],
      "source": [
        "def sample_model(model):\n",
        "    #sample 8 x 8 embedding vectors\n",
        "    encoding_indices = torch.argmin(torch.rand(size = [8 * 8, n_embeddings]), dim=1).to(device).unsqueeze(1)\n",
        "    encodings = torch.zeros(encoding_indices.shape[0], n_embeddings, device=device)\n",
        "    encodings.scatter_(1, encoding_indices, 1)\n",
        "    quantized = torch.matmul(encodings, model.vector_quantization._embedding.weight).view(1, 8, 8, 64)\n",
        "    quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
        "    z_e = model.decoder(quantized)\n",
        "    return z_e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swOhy50pSSU4",
        "outputId": "ef99304e-37aa-440b-e774-0cb49fa3f5b2"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(sample_model(model)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ],
      "source": [
        "plt.imshow(sample_model(vqvae).squeeze(0).permute(1, 2, 0).cpu().detach() + 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYTPDKSU4EuV"
      },
      "outputs": [],
      "source": [
        "dl = torch.utils.data.DataLoader(training_data,batch_size=1024,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DgRE31N33V1"
      },
      "outputs": [],
      "source": [
        "out = None\n",
        "for data, _ in dl:\n",
        "    data = data.to(device)\n",
        "    loss, x_hat, perplexity, encodings = vqvae.vector_quantization(vqvae.pre_quantization_conv(vqvae.encoder(data)))\n",
        "    if out == None:\n",
        "        out = x_hat\n",
        "    else:\n",
        "        out = torch.cat([out, x_hat], dim = 0) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSaVfiRb4cyQ"
      },
      "outputs": [],
      "source": [
        "x_hat.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-ZpJG2OUl5y"
      },
      "source": [
        "### Concept Check 3.2.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqYdj12VrOwB"
      },
      "source": [
        "## Ablations & Visualizations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOS3tJZ2wI3V"
      },
      "outputs": [],
      "source": [
        "#Michael, when you make the student copy, replace the below with a comment that says copy over add_noise function from ae_vae notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZntPH3tLv9y3"
      },
      "outputs": [],
      "source": [
        "def add_noise(tensor, mean=0., std=1., noise_weight=0.5):\n",
        "    noise = torch.randn(tensor.size()) * std + mean\n",
        "    return torch.clip(tensor + noise_weight * noise, 0., 1.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PTk-7uFuhxh"
      },
      "source": [
        "### Quick AE Implementation Using Same Encoder-Decoder Architecture as VQ-VAE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKVNE0PVwSZv"
      },
      "outputs": [],
      "source": [
        "# AE/VAE hyperparams\n",
        "batch_size = 64\n",
        "n_hiddens = 64\n",
        "n_residual_hiddens = 32\n",
        "n_residual_layers = 2\n",
        "embedding_dim = 64\n",
        "code_size=8\n",
        "#n_embeddings = 512\n",
        "#beta = .25\n",
        "#lr = 3e-4\n",
        "\n",
        "epochs = 25\n",
        "lr = 1e-3\n",
        "noise=False\n",
        "lin_dim=256\n",
        "regularization_weight = .0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIIL5Tl7ufve"
      },
      "outputs": [],
      "source": [
        "class AE(nn.Module):\n",
        "    def __init__(self, h_dim, res_h_dim, n_res_layers, embedding_dim, lin_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.h_dim = h_dim\n",
        "        # encode image into continuous latent space\n",
        "        self.encoder = Encoder(3, h_dim, n_res_layers, res_h_dim)\n",
        "\n",
        "        #FC Projections\n",
        "        self.fc1 = nn.Linear(h_dim*code_size*code_size, lin_dim)\n",
        "        self.fc2 = nn.Linear(lin_dim, h_dim*code_size*code_size)\n",
        "\n",
        "        # decode the discrete latent representation\n",
        "        self.decoder = Decoder(embedding_dim, h_dim, n_res_layers, res_h_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = x.view(batch_size, self.h_dim*code_size*code_size)\n",
        "        return self.fc1(x)\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = self.fc2(z)\n",
        "        z = z.view(batch_size, self.h_dim, code_size, code_size)\n",
        "        return self.decoder(z)\n",
        "    \n",
        "    def train_step(self, optimizer, x_in, x_star):\n",
        "        z = self.encode(x_in)\n",
        "        x_hat = self.decode(z)\n",
        "\n",
        "        loss = self.loss(x_star, x_hat)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, x):\n",
        "        z = self.encode(x)\n",
        "        x_hat = self.decode(z)\n",
        "        \n",
        "        loss = self.loss(x, x_hat)\n",
        "        return loss\n",
        "    @staticmethod\n",
        "    def loss(x, x_hat):\n",
        "        #Mean-Squared Error Reconstruction Loss\n",
        "        criterion = nn.MSELoss()\n",
        "        return criterion(x, x_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "TSJ33rhcvBXD",
        "outputId": "a2aec1d3-ce59-4fa8-9b96-2497f5cfc73d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▍         | 37/782 [00:02<00:54, 13.56it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-ff6c041ef432>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mx_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_no_noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_star\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_no_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mae_train_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#loss every iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-412cd80a97c1>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, optimizer, x_in, x_star)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_star\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-412cd80a97c1>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_dim\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcode_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcode_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-f7191c6412a0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-b4d2d874763e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-7ca29c6fed43>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "ae = AE(n_hiddens, n_residual_hiddens, n_residual_layers, embedding_dim, lin_dim).to(device)\n",
        "optimizer = torch.optim.Adam(ae.parameters(), lr=lr)\n",
        "\n",
        "# train\n",
        "ae_train_losses = []\n",
        "ae_test_losses = []\n",
        "step = 0\n",
        "report_every = 500\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}\")\n",
        "    #train loss:\n",
        "    for x, y in tqdm(train_loader):\n",
        "        x_no_noise = x.to(device)\n",
        "        if noise:\n",
        "            x_in = add_noise(x_no_noise, noise_weight=0.5)\n",
        "        else:\n",
        "            x_in = x_no_noise\n",
        "        loss = ae.train_step(optimizer, x_in=x_in, x_star=x_no_noise)\n",
        "        ae_train_losses.append(loss.cpu().detach().numpy()) #loss every iteration\n",
        "        step += 1\n",
        "        if step % report_every == 0:\n",
        "            print(f\"Training loss: {loss}\")\n",
        "    #ae_train_losses.append(loss.detach().numpy()) #loss after every epoch\n",
        "    #test loss\n",
        "    for b, (X_test, y_test) in enumerate(validation_loader):\n",
        "        X_test = X_test.to(device)\n",
        "        loss = ae.test_step(X_test)\n",
        "        ae_test_losses.append(loss.cpu().detach().numpy()) #loss every iteration\n",
        "    #ae_test_losses.append(loss.detach().numpy()) #loss after every epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghRxNmUh1q3V"
      },
      "source": [
        "### Quick VAE Implementation Using Same Encoder-Decoder Architecture as VQ-VAE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdCxn2fK1txe"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, h_dim, res_h_dim, n_res_layers, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.z_mean = Encoder(3, h_dim, n_res_layers, res_h_dim)\n",
        "        self.z_log_std = Encoder(3, h_dim, n_res_layers, res_h_dim)\n",
        "        self.decoder = Decoder(embedding_dim, h_dim, n_res_layers, res_h_dim)\n",
        "\n",
        "        self.h_dim = h_dim\n",
        "        #FC Projections\n",
        "        self.fc1 = nn.Linear(h_dim*code_size*code_size, lin_dim)\n",
        "        self.fc2 = nn.Linear(lin_dim, h_dim*code_size*code_size)\n",
        "    \n",
        "    def _encode(self, x):\n",
        "        print(x.shape)\n",
        "        x = self.z_mean(x)\n",
        "        x = x.view(batch_size, self.h_dim*code_size*code_size)\n",
        "        z_mean = self.fc1(x)\n",
        "        \n",
        "\n",
        "        x = self.z_mean(x)\n",
        "        x = x.view(batch_size, self.h_dim*code_size*code_size)\n",
        "        z_log_std = self.fc1(x)\n",
        "\n",
        "        # reparameterization trick\n",
        "        z_std = torch.exp(z_log_std)\n",
        "        eps = torch.randn_like(z_std)\n",
        "        z = z_mean + eps * z_std\n",
        "\n",
        "        # log prob\n",
        "        # 'd' not sampled on purpose\n",
        "        # to show reparameterization trick\n",
        "        d = Independent(Normal(z_mean, z_std), 1)\n",
        "        log_prob = d.log_prob(z)\n",
        "        \n",
        "        return z_mean + eps * z_std, log_prob\n",
        "    \n",
        "    def encode(self, x):\n",
        "        z, _ = self._encode(x)\n",
        "        return z\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = self.fc2(z)\n",
        "        z = z.view(batch_size, self.h_dim, code_size, code_size)\n",
        "        return self.decoder(z)\n",
        "    \n",
        "    def train_step(self, optimizer, x_in, x_star):\n",
        "        z, log_prob = self._encode(x_in)\n",
        "        x_hat = self.decode(z)\n",
        "        loss = self.loss(x_star, x_hat, z, log_prob)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, x):\n",
        "        z, log_prob = self._encode(x)\n",
        "        x_hat = self.decode(z)\n",
        "        \n",
        "        loss = self.loss(x, x_hat, z, log_prob)\n",
        "        return loss\n",
        "\n",
        "    @staticmethod\n",
        "    def loss(x, x_hat, z, log_prob, kl_weight=regularization_weight):\n",
        "        criterion = nn.MSELoss()\n",
        "        reconst_loss = criterion(x, x_hat)\n",
        "\n",
        "        z_dim = z.shape[-1]\n",
        "        standard_normal = MultivariateNormal(torch.zeros(z_dim).to(device), \n",
        "                                             torch.eye(z_dim).to(device))\n",
        "        #print(MultivariateNormal.device)\n",
        "        kld_loss = (log_prob - standard_normal.log_prob(z)).mean()\n",
        "        \n",
        "        return reconst_loss + kl_weight * kld_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNHf8PDJ8kiO"
      },
      "outputs": [],
      "source": [
        "#vae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UNOM9n2F1t9Y",
        "outputId": "8321b003-a63b-47dc-dbdf-ddab491a5313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/782 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 3, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-95a00e75c18f>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mx_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_no_noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_star\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_no_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mvae_train_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#loss every iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-57017e5dea0e>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, optimizer, x_in, x_star)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_star\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"x_in {x_in.device} x_star {x_star.device} Log_prob dev {log_prob.device} z {z.device} \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-57017e5dea0e>\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_dim\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcode_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcode_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mz_log_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-f7191c6412a0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [64, 4096]"
          ]
        }
      ],
      "source": [
        "vae = VAE(n_hiddens, n_residual_hiddens, n_residual_layers, embedding_dim).to(device)\n",
        "optimizer = torch.optim.Adam(vae.parameters(), lr=lr)\n",
        "\n",
        "# train\n",
        "vae_train_losses = []\n",
        "vae_test_losses = []\n",
        "step = 0\n",
        "report_every = 500\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}\")\n",
        "    #train loss:\n",
        "    for x, y in tqdm(train_loader):\n",
        "        x_no_noise = x.to(device)\n",
        "        if noise:\n",
        "            x_in = add_noise(x_no_noise, noise_weight=0.5)\n",
        "        else:\n",
        "            x_in = x_no_noise\n",
        "        loss = vae.train_step(optimizer, x_in=x_in, x_star=x_no_noise)\n",
        "        vae_train_losses.append(loss.cpu().detach().numpy()) #loss every iteration\n",
        "        step += 1\n",
        "        if step % report_every == 0:\n",
        "            print(f\"Training loss: {loss}\")\n",
        "    #vae_train_losses.append(loss.detach().numpy()) #loss after every epoch\n",
        "    #test loss\n",
        "    for b, (X_test, y_test) in enumerate(validation_loader):\n",
        "        X_test = X_test.to(device)\n",
        "        loss = vae.test_step(X_test)\n",
        "        vae_test_losses.append(loss.cpu().detach().numpy()) #loss every iteration\n",
        "    #vae_test_losses.append(loss.detach().numpy()) #loss after every epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg3ZiwIe4ojg"
      },
      "source": [
        "### 1. Loss Visualization (AE vs. VAE vs. VQVAE):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS1f1u1kREOL"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    fig, ax = plt.subplots(2, figsize=(8,10))\n",
        "    ax[0].plot(ae_train_losses, label=\"AE\")\n",
        "    ax[0].plot(vae_train_losses, label=\"VAE\")\n",
        "    ax[0].plot(train_res_recon_error, label=\"VQ-VAE\")\n",
        "    ax[0].set_title(\"Train Losses\")\n",
        "    ax[0].set_ylabel(\"Train Loss\")\n",
        "    ax[0].set_xlabel(\"Iteration\")\n",
        "    ax[0].legend()\n",
        "    \n",
        "    ax[1].plot(ae_test_losses, label=\"AE\")\n",
        "    ax[1].plot(vae_test_losses, label=\"VAE\")\n",
        "    ax[1].plot(test_res_recon_error, label=\"VQ-VAE\")\n",
        "    ax[1].set_title(\"Test Losses\")\n",
        "    ax[1].set_ylabel(\"Test Loss\")\n",
        "    ax[1].set_xlabel(\"Iteration\")\n",
        "    ax[1].legend()\n",
        "\n",
        "#Note: if you want per epoch (or avg. epoch) losses, you will have to change the above code somewhat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxF9Z6bjrEUS"
      },
      "source": [
        "### 2. Reconstruction Visualization (AE vs. VAE vs. VQVAE):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "MJp6wseCRnqj",
        "outputId": "3d0d1ca5-de24-4481-d814-274cf02cdf03"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-d7f01743195b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvqvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#Grab first batch of images:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vqvae' is not defined"
          ]
        }
      ],
      "source": [
        "#Visualize the samples in bulk:\n",
        "ae.eval()\n",
        "vae.eval()\n",
        "vqvae.eval()\n",
        "with torch.no_grad():\n",
        "    #Grab first batch of images:\n",
        "    for images, labels in validation_loader:\n",
        "        recon_ae = ae.decode(ae.encode(images))\n",
        "        recon_vae = vae.decode(vae.encode(images))\n",
        "        _, recon_vqvae, _ = vqvae(images)\n",
        "        break\n",
        "\n",
        "\n",
        "    #Print and show the first 10 samples:\n",
        "\n",
        "    print(f\"Labels: {labels[0:10]}\")\n",
        "    im = make_grid(images[:10], nrow=10)\n",
        "    ae_im = make_grid(recon_ae[:10], nrow=10)\n",
        "    vae_im = make_grid(recon_vae[:10], nrow=10)\n",
        "    vqvae_im = make_grid(recon_vqvae[:10], nrow=10)\n",
        "\n",
        "    fig, ax = plt.subplots(4, figsize=(45,4.5))\n",
        "    fig.tight_layout(pad=1.5)\n",
        "    ax[0].imshow(np.transpose(im.numpy(), (1, 2, 0))) #Remember that default MNIST data is CWH, but matplotlib uses WHC\n",
        "    ax[0].set_title(\"Original:\")\n",
        "\n",
        "    ax[1].imshow(np.transpose(ae_im.numpy(), (1, 2, 0)))\n",
        "    ax[1].set_title(\"AE Reconstruction:\")\n",
        "    \n",
        "    ax[2].imshow(np.transpose(vae_im.numpy(), (1, 2, 0)))\n",
        "    ax[2].set_title(\"VAE Reconstruction:\")\n",
        "\n",
        "    ax[2].imshow(np.transpose(vqvae_im.numpy(), (1, 2, 0)))\n",
        "    ax[2].set_title(\"VQ-VAE Reconstruction:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQt62NQtrEUS"
      },
      "source": [
        "### 3. Denoising Ablation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP5F8fxCTTmg"
      },
      "source": [
        "Now, let us see the denoising capabilities of these architectures. Go back to the function add_noise in order to add some Gaussian noise to the training data, $X$. You can just copy-paste this from the AE-VAE notebook and set the noise hyperparameter to True. Then, apply this noise to the sample when training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBwLby0sTvmR"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    fig, ax = plt.subplots(2, figsize=(8,10))\n",
        "    ax[0].plot(ae_train_losses, label=\"AE\")\n",
        "    ax[0].plot(vae_train_losses, label=\"VAE\")\n",
        "    ax[0].plot(train_res_recon_error, label=\"VQ-VAE\")\n",
        "    ax[0].set_title(\"Train Losses\")\n",
        "    ax[0].set_ylabel(\"Train Loss\")\n",
        "    ax[0].set_xlabel(\"Iteration\")\n",
        "    ax[0].legend()\n",
        "    \n",
        "    ax[1].plot(ae_test_losses, label=\"AE\")\n",
        "    ax[1].plot(vae_test_losses, label=\"VAE\")\n",
        "    ax[1].plot(test_res_recon_error, label=\"VQ-VAE\")\n",
        "    ax[1].set_title(\"Test Losses\")\n",
        "    ax[1].set_ylabel(\"Test Loss\")\n",
        "    ax[1].set_xlabel(\"Iteration\")\n",
        "    ax[1].legend()\n",
        "\n",
        "#Note: if you want per epoch (or avg. epoch) losses, you will have to change the above code somewhat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w6rOnxHTv_-"
      },
      "outputs": [],
      "source": [
        "#Visualize the samples in bulk:\n",
        "ae.eval()\n",
        "vae.eval()\n",
        "vqvae.eval()\n",
        "with torch.no_grad():\n",
        "    #Grab first batch of images:\n",
        "    for images, labels in validation_loader:\n",
        "        recon_ae = ae.decode(ae.encode(images))\n",
        "        recon_vae = vae.decode(vae.encode(images))\n",
        "        _, recon_vqvae, _ = vqvae(images)\n",
        "        break\n",
        "\n",
        "\n",
        "    #Print and show the first 10 samples:\n",
        "\n",
        "    print(f\"Labels: {labels[0:10]}\")\n",
        "    im = make_grid(images[:10], nrow=10)\n",
        "    ae_im = make_grid(recon_ae[:10], nrow=10)\n",
        "    vae_im = make_grid(recon_vae[:10], nrow=10)\n",
        "    vqvae_im = make_grid(recon_vqvae[:10], nrow=10)\n",
        "\n",
        "    fig, ax = plt.subplots(4, figsize=(45,4.5))\n",
        "    fig.tight_layout(pad=1.5)\n",
        "    ax[0].imshow(np.transpose(im.numpy(), (1, 2, 0))) #Remember that default MNIST data is CWH, but matplotlib uses WHC\n",
        "    ax[0].set_title(\"Original:\")\n",
        "\n",
        "    ax[1].imshow(np.transpose(ae_im.numpy(), (1, 2, 0)))\n",
        "    ax[1].set_title(\"AE Reconstruction:\")\n",
        "    \n",
        "    ax[2].imshow(np.transpose(vae_im.numpy(), (1, 2, 0)))\n",
        "    ax[2].set_title(\"VAE Reconstruction:\")\n",
        "\n",
        "    ax[2].imshow(np.transpose(vqvae_im.numpy(), (1, 2, 0)))\n",
        "    ax[2].set_title(\"VQ-VAE Reconstruction:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDjJHYByrEUS"
      },
      "source": [
        "### 4. Latent Space Dimension & Size Ablation (Jason, if time; this might be more non-trivial than we initially thought):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7djQtPlrEUS"
      },
      "source": [
        "### 5. Sample Generation Ablation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS1_y6Hy-dJ8"
      },
      "source": [
        "**Sampling**\n",
        "\n",
        "Unlike VAEs, VQ-VAEs can be difficult to sample from. Recall in VAEs, we enforced $p(z) \\sim N(0, I)$ using a KL divergence loss. This makes it easy to sample a VAE. But VQ-VAEs don't have this nice property. \n",
        "\n",
        "To see this, we can try sampling $z$ uniformly, and see the resulting image. To be more explicit, for each latent pixel in the 8x8 space, we sample one of the 512 codebook vectors uniformly, then feed the whole thing into the decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00J9XII0-dJ8",
        "outputId": "9eb7b0d8-234b-437c-82bc-59fcf0ccdccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64 32 2 512 64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# first load the model again\n",
        "n_embeddings = 512\n",
        "print(n_hiddens,n_residual_hiddens, n_residual_layers,n_embeddings,embedding_dim)\n",
        "model = VQVAE(n_hiddens, n_residual_hiddens, n_residual_layers,\n",
        "              n_embeddings, embedding_dim, \n",
        "              beta).to(device)\n",
        "model.load_state_dict(torch.load('VQ-VAE_Model'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "-H4henLD-dJ9",
        "outputId": "6a3ec31d-b10c-4f66-f0ba-32cdff83410a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd241283e20>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def sample_model_uniform(model):\n",
        "  # sample 8 x 8 embedding vectors\n",
        "  encoding_indices = torch.argmin(torch.rand(size = [8 * 8, n_embeddings]), dim=1).to(device).unsqueeze(1)\n",
        "  encodings = torch.zeros(encoding_indices.shape[0], n_embeddings, device=device)\n",
        "  encodings.scatter_(1, encoding_indices, 1)\n",
        "  quantized = torch.matmul(encodings, model.vector_quantization._embedding.weight)\n",
        "  quantized = quantized.view(1, 8, 8, 64)\n",
        "  quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
        "  z_e = model.decoder(quantized)\n",
        "  return z_e\n",
        "\n",
        "plt.imshow(sample_model_uniform(model).squeeze(0).permute(1, 2, 0).cpu().detach() + 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dxQczme-dJ-"
      },
      "source": [
        "Chances are the generated image is difficult to recognize. We have 512 codebook vectors, each of which can go into 1 of the squares in the 8 by 8 embedding. So despite being a discrete representation, there are some $64^{512} \\approx 10^{900}$ possible things that can be encoded. So it makes sense that sampling uniformly in this space likely won't reveal anything meaningful. \n",
        "\n",
        "Run the code below to see an example of the embedding index per latent pixel. Do this a couple times for different images. What do you see, are there patterns you notice?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "KMhZDTPt-dJ-",
        "outputId": "0d7915b2-4d39-48d8-8ece-b4976e28c871"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# vqvae latents\n",
        "viz_loader = torch.utils.data.DataLoader(training_data, batch_size = 1, shuffle = True)\n",
        "X, _ = next(iter(viz_loader))\n",
        "X = X.to(device)\n",
        "z_e = model.encoder(X)\n",
        "z_e = model.pre_quantization_conv(z_e)\n",
        "_, z_q, _, encodings = model.vector_quantization(z_e)\n",
        "plt.imshow(encodings.cpu().numpy(), cmap='gray', aspect='auto')\n",
        "plt.xlabel('embedding index')\n",
        "plt.ylabel('latent pixel')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cCX6nd8-dJ_"
      },
      "source": [
        "**Train an autoregressive model**\n",
        "\n",
        "From the visualizations above, it should be relatively clear that the embeddings chosen per pixel are neither uniformly distributed, nor independent. We'd been breaking apart $p(z_1,\\ldots, z_n) = \\prod_{i=1}^n p(z_i)$. What would make the generation better would be to learn a conditional distribution of each latent pixel based on the previous ones: $p(z_k | z_{k-1} \\ldots z_1)$. In the original VQ-VAE paper, this is done using an autoregressive model, PixelCNN. Here, we attempt to do the same thing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "hIHM6yor-dJ_"
      },
      "outputs": [],
      "source": [
        "# PixelCNNs aren't the focus of this project.\n",
        "# This was taken from: https://github.com/jzbontar/pixelcnn-pytorch/\n",
        "\n",
        "class MaskedConv2d(nn.Conv2d):\n",
        "    def __init__(self, mask_type, *args, **kwargs):\n",
        "        super(MaskedConv2d, self).__init__(*args, **kwargs)\n",
        "        assert mask_type in {'A', 'B'}\n",
        "        self.register_buffer('mask', self.weight.data.clone())\n",
        "        _, _, kH, kW = self.weight.size()\n",
        "        self.mask.fill_(1)\n",
        "        self.mask[:, :, kH // 2, kW // 2 + (mask_type == 'B'):] = 0\n",
        "        self.mask[:, :, kH // 2 + 1:] = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.weight.data *= self.mask\n",
        "        return super(MaskedConv2d, self).forward(x)\n",
        "\n",
        "fm = 64\n",
        "pixCNN = nn.Sequential(\n",
        "    MaskedConv2d('A', 512,  fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
        "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
        "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
        "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
        "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
        "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
        "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
        "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
        "    nn.Conv2d(fm, 512, 1)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "6EK4XxBZ-dKA",
        "outputId": "83a27402-1d72-428f-b091-e50b02827a22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:30<00:00, 12.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6074960231781006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▌      | 141/391 [00:10<00:19, 12.87it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-793f2b76e228>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_inds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 state_steps)\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    282\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_mul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_exp_avg_sqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_addcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_exp_avg_sqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcapturable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "batch_size = 128\n",
        "epochs = 15\n",
        "\n",
        "optimizer = torch.optim.Adam(pixCNN.parameters(), lr=lr)\n",
        "train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "train_losses = []\n",
        "for _ in range(epochs):\n",
        "    for X, _ in tqdm(train_loader):\n",
        "        if X.shape[0] != batch_size:\n",
        "            continue\n",
        "\n",
        "        # get our embedding\n",
        "        X = X.to(device)\n",
        "        z_e = model.encoder(X)\n",
        "        z_e = model.pre_quantization_conv(z_e)\n",
        "        _, z_q, _, encodings = model.vector_quantization(z_e) \n",
        "\n",
        "        enc_inds = encodings.argmax(dim=1, keepdims=True)\n",
        "        encodings_img = encodings.reshape(batch_size, 8, 8, -1)\n",
        "        encodings_img = encodings_img.permute(0, 3, 1, 2).float().to(device)\n",
        "\n",
        "        # learn\n",
        "        optimizer.zero_grad()\n",
        "        output = pixCNN(encodings_img)\n",
        "\n",
        "        output = output.permute(0, 2, 3, 1)\n",
        "        output = output.reshape(-1, 512)\n",
        "\n",
        "        loss = F.cross_entropy(output, enc_inds.squeeze(1).long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "    print(train_losses[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otfVO_n5-dKB",
        "outputId": "381d263f-f2e3-4d38-98a3-2ea6c78e3cc6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "torch.save(pixCNN.state_dict(), 'PixCNN.pt')\n",
        "pixCNN.load_state_dict(torch.load('PixCNN.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "RTtyYzZp-dKB",
        "outputId": "af51b8b7-44fc-4636-fc46-0613d458aee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-42b2b4ccc1eb>:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  weights = F.softmax(output[:, :, i, j])\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sample = torch.zeros(1, 512, 8, 8).to(device)\n",
        "for i in range(8):\n",
        "    for j in range(8):\n",
        "        output = pixCNN(sample)\n",
        "        weights = F.softmax(output[:, :, i, j])\n",
        "        embed_indx = torch.multinomial(weights, 1).item()\n",
        "        sample[:, embed_indx, i, j] = 1\n",
        "\n",
        "sample = sample.squeeze(0).reshape(512, 64)\n",
        "sample = sample.permute((1, 0))\n",
        "\n",
        "quantized = torch.matmul(sample, model.vector_quantization._embedding.weight)\n",
        "quantized = quantized.view(1, 8, 8, 64)\n",
        "quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
        "z_e = model.decoder(quantized)\n",
        "\n",
        "plt.imshow(z_e.squeeze(0).permute(1, 2, 0).cpu().detach() + 0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDtYitRNUAX9"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y73MCRnzUGxF"
      },
      "source": [
        "Any references utilized in this project can be found in the README of our repo"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ghRxNmUh1q3V",
        "Dg3ZiwIe4ojg",
        "JxF9Z6bjrEUS",
        "mQt62NQtrEUS"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}