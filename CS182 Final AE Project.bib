
@article{hinton_reducing_2006,
	title = {Reducing the {Dimensionality} of {Data} with {Neural} {Networks}},
	volume = {313},
	url = {https://www.science.org/doi/10.1126/science.1127647},
	doi = {10.1126/science.1127647},
	abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
	number = {5786},
	urldate = {2023-04-08},
	journal = {Science},
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	month = jul,
	year = {2006},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {504--507},
	file = {Full Text PDF:/Users/cmitra/Zotero/storage/GPUJ85QF/Hinton and Salakhutdinov - 2006 - Reducing the Dimensionality of Data with Neural Ne.pdf:application/pdf},
}

@article{vincent_stacked_nodate,
	title = {Stacked {Denoising} {Autoencoders}: {Learning} {Useful} {Representations} in a {Deep} {Network} with a {Local} {Denoising} {Criterion}},
	abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classiﬁcation problems to yield signiﬁcantly lower classiﬁcation error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classiﬁers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
	language = {en},
	author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	file = {Vincent et al. - Stacked Denoising Autoencoders Learning Useful Re.pdf:/Users/cmitra/Zotero/storage/UBPMACR2/Vincent et al. - Stacked Denoising Autoencoders Learning Useful Re.pdf:application/pdf},
}

@article{torabi_practical_2023,
	title = {Practical autoencoder based anomaly detection by using vector reconstruction error},
	volume = {6},
	issn = {2523-3246},
	url = {https://doi.org/10.1186/s42400-022-00134-9},
	doi = {10.1186/s42400-022-00134-9},
	abstract = {Nowadays, cloud computing provides easy access to a set of variable and configurable computing resources based on user demand through the network. Cloud computing services are available through common internet protocols and network standards. In addition to the unique benefits of cloud computing, insecure communication and attacks on cloud networks cannot be ignored. There are several techniques for dealing with network attacks. To this end, network anomaly detection systems are widely used as an effective countermeasure against network anomalies. The anomaly-based approach generally learns normal traffic patterns in various ways and identifies patterns of anomalies. Network anomaly detection systems have gained much attention in intelligently monitoring network traffic using machine learning methods. This paper presents an efficient model based on autoencoders for anomaly detection in cloud computing networks. The autoencoder learns a basic representation of the normal data and its reconstruction with minimum error. Therefore, the reconstruction error is used as an anomaly or classification metric. In addition, to detecting anomaly data from normal data, the classification of anomaly types has also been investigated. We have proposed a new approach by examining an autoencoder’s anomaly detection method based on data reconstruction error. Unlike the existing autoencoder-based anomaly detection techniques that consider the reconstruction error of all input features as a single value, we assume that the reconstruction error is a vector. This enables our model to use the reconstruction error of every input feature as an anomaly or classification metric. We further propose a multi-class classification structure to classify the anomalies. We use the CIDDS-001 dataset as a commonly accepted dataset in the literature. Our evaluations show that the performance of the proposed method has improved considerably compared to the existing ones in terms of accuracy, recall, false-positive rate, and F1-score metrics.},
	number = {1},
	urldate = {2023-04-23},
	journal = {Cybersecurity},
	author = {Torabi, Hasan and Mirtaheri, Seyedeh Leili and Greco, Sergio},
	month = jan,
	year = {2023},
	keywords = {Anomaly detection, Autoencoder, Cloud, Machine learning, Practical, Reconstruction error},
	pages = {1},
	file = {Full Text PDF:/Users/cmitra/Zotero/storage/MLVRGKBV/Torabi et al. - 2023 - Practical autoencoder based anomaly detection by u.pdf:application/pdf;Snapshot:/Users/cmitra/Zotero/storage/HY2R36Z5/s42400-022-00134-9.html:text/html},
}

@misc{oord_neural_2018,
	title = {Neural {Discrete} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1711.00937},
	abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
	urldate = {2023-04-23},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
	month = may,
	year = {2018},
	note = {arXiv:1711.00937 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/cmitra/Zotero/storage/KW7R3387/1711.html:text/html;Full Text PDF:/Users/cmitra/Zotero/storage/GZMPS7X7/Oord et al. - 2018 - Neural Discrete Representation Learning.pdf:application/pdf},
}

@misc{dertat_applied_2017,
	title = {Applied {Deep} {Learning} - {Part} 3: {Autoencoders}},
	shorttitle = {Applied {Deep} {Learning} - {Part} 3},
	url = {https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798},
	abstract = {Overview},
	language = {en},
	urldate = {2023-04-23},
	journal = {Medium},
	author = {Dertat, Arden},
	month = oct,
	year = {2017},
}

@article{sahai_lecture_nodate,
	title = {Lecture 16: {Self}-{Supervision} and {Autoencoders} (cont.)},
	language = {en},
	author = {Sahai, Anant and Yang, Xingyi and Seal, Sayan},
	file = {Sahai et al. - Lecture 16 Self-Supervision and Autoencoders (con.pdf:/Users/cmitra/Zotero/storage/LWZALAZL/Sahai et al. - Lecture 16 Self-Supervision and Autoencoders (con.pdf:application/pdf},
}

@misc{hazarika_misa_2020,
	title = {{MISA}: {Modality}-{Invariant} and -{Specific} {Representations} for {Multimodal} {Sentiment} {Analysis}},
	shorttitle = {{MISA}},
	url = {http://arxiv.org/abs/2005.03545},
	abstract = {Multimodal Sentiment Analysis is an active area of research that leverages multimodal signals for affective understanding of user-generated videos. The predominant approach, addressing this task, has been to develop sophisticated fusion techniques. However, the heterogeneous nature of the signals creates distributional modality gaps that pose significant challenges. In this paper, we aim to learn effective modality representations to aid the process of fusion. We propose a novel framework, MISA, which projects each modality to two distinct subspaces. The first subspace is modality-invariant, where the representations across modalities learn their commonalities and reduce the modality gap. The second subspace is modality-specific, which is private to each modality and captures their characteristic features. These representations provide a holistic view of the multimodal data, which is used for fusion that leads to task predictions. Our experiments on popular sentiment analysis benchmarks, MOSI and MOSEI, demonstrate significant gains over state-of-the-art models. We also consider the task of Multimodal Humor Detection and experiment on the recently proposed UR\_FUNNY dataset. Here too, our model fares better than strong baselines, establishing MISA as a useful multimodal framework.},
	urldate = {2023-04-23},
	publisher = {arXiv},
	author = {Hazarika, Devamanyu and Zimmermann, Roger and Poria, Soujanya},
	month = oct,
	year = {2020},
	note = {arXiv:2005.03545 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted at ACM MM 2020 (Oral Paper)},
	file = {arXiv.org Snapshot:/Users/cmitra/Zotero/storage/PPP9FJDD/2005.html:text/html;Full Text PDF:/Users/cmitra/Zotero/storage/PT3F2HXP/Hazarika et al. - 2020 - MISA Modality-Invariant and -Specific Representat.pdf:application/pdf},
}

@misc{rocca_understanding_2021,
	title = {Understanding {Variational} {Autoencoders} ({VAEs})},
	url = {https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73},
	abstract = {Building, step by step, the reasoning that leads to VAEs.},
	language = {en},
	urldate = {2023-04-23},
	journal = {Medium},
	author = {Rocca, Joseph},
	month = mar,
	year = {2021},
	file = {Snapshot:/Users/cmitra/Zotero/storage/DJR7LSQI/understanding-variational-autoencoders-vaes-f70510919f73.html:text/html},
}

@misc{noauthor_denoising_nodate,
	title = {Denoising {MNIST} images using autoencoder},
	url = {https://kaggle.com/code/theblackmamba31/denoising-mnist-images-using-autoencoder},
	abstract = {Explore and run machine learning code with Kaggle Notebooks {\textbar} Using data from No attached data sources},
	language = {en},
	urldate = {2023-04-23},
	file = {Snapshot:/Users/cmitra/Zotero/storage/HPLJ4RWZ/denoising-mnist-images-using-autoencoder.html:text/html},
}

@misc{dobilas_vae_2022,
	title = {{VAE}: {Variational} {Autoencoders} — {How} to {Employ} {Neural} {Networks} to {Generate} {New} {Images}},
	shorttitle = {{VAE}},
	url = {https://towardsdatascience.com/vae-variational-autoencoders-how-to-employ-neural-networks-to-generate-new-images-bdeb216ed2c0},
	abstract = {An overview of VAEs with a complete Python example that teaches you how to build one yourself},
	language = {en},
	urldate = {2023-04-23},
	journal = {Medium},
	author = {Dobilas, Saul},
	month = may,
	year = {2022},
	file = {Snapshot:/Users/cmitra/Zotero/storage/C6B38JRD/vae-variational-autoencoders-how-to-employ-neural-networks-to-generate-new-images-bdeb216ed2c0.html:text/html},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2023-04-24},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
	file = {arXiv.org Snapshot:/Users/cmitra/Zotero/storage/49D4B24N/2010.html:text/html;Full Text PDF:/Users/cmitra/Zotero/storage/C6TG3EZ5/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf},
}

@misc{anello_convolutional_2022,
	title = {Convolutional {Autoencoder} in {Pytorch} on {MNIST} dataset},
	url = {https://medium.com/dataseries/convolutional-autoencoder-in-pytorch-on-mnist-dataset-d65145c132ac},
	abstract = {The post is the seventh in a series of guides to build deep learning models with Pytorch. Below, there is the full series:},
	language = {en},
	urldate = {2023-04-24},
	journal = {DataSeries},
	author = {Anello, Eugenia},
	month = dec,
	year = {2022},
	file = {Snapshot:/Users/cmitra/Zotero/storage/HM5DXXF9/convolutional-autoencoder-in-pytorch-on-mnist-dataset-d65145c132ac.html:text/html},
}

@misc{noauthor_ch_nodate,
	title = {Ch. 9 - {Object} {Detection} and {Segmentation}},
	url = {https://manipulation.csail.mit.edu/segmentation.html},
	urldate = {2023-04-24},
	file = {Ch. 9 - Object Detection and Segmentation:/Users/cmitra/Zotero/storage/6TT76WTM/segmentation.html:text/html},
}

@misc{gu_vector_2022,
	title = {Vector {Quantized} {Diffusion} {Model} for {Text}-to-{Image} {Synthesis}},
	url = {http://arxiv.org/abs/2111.14822},
	abstract = {We present the vector quantized diffusion (VQ-Diffusion) model for text-to-image generation. This method is based on a vector quantized variational autoencoder (VQ-VAE) whose latent space is modeled by a conditional variant of the recently developed Denoising Diffusion Probabilistic Model (DDPM). We find that this latent-space method is well-suited for text-to-image generation tasks because it not only eliminates the unidirectional bias with existing methods but also allows us to incorporate a mask-and-replace diffusion strategy to avoid the accumulation of errors, which is a serious problem with existing methods. Our experiments show that the VQ-Diffusion produces significantly better text-to-image generation results when compared with conventional autoregressive (AR) models with similar numbers of parameters. Compared with previous GAN-based text-to-image methods, our VQ-Diffusion can handle more complex scenes and improve the synthesized image quality by a large margin. Finally, we show that the image generation computation in our method can be made highly efficient by reparameterization. With traditional AR methods, the text-to-image generation time increases linearly with the output image resolution and hence is quite time consuming even for normal size images. The VQ-Diffusion allows us to achieve a better trade-off between quality and speed. Our experiments indicate that the VQ-Diffusion model with the reparameterization is fifteen times faster than traditional AR methods while achieving a better image quality.},
	urldate = {2023-04-24},
	publisher = {arXiv},
	author = {Gu, Shuyang and Chen, Dong and Bao, Jianmin and Wen, Fang and Zhang, Bo and Chen, Dongdong and Yuan, Lu and Guo, Baining},
	month = mar,
	year = {2022},
	note = {arXiv:2111.14822 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/cmitra/Zotero/storage/FP3SSQXC/2111.html:text/html;Full Text PDF:/Users/cmitra/Zotero/storage/A2LLZD55/Gu et al. - 2022 - Vector Quantized Diffusion Model for Text-to-Image.pdf:application/pdf},
}

@misc{hayrice_how_2018,
	title = {How to {Train} {Your} {ResNet}},
	url = {https://myrtle.ai/learn/how-to-train-your-resnet/},
	abstract = {The introduction to a series of posts investigating how to train Residual networks efficiently on the CIFAR10 image classification dataset. By the fourth post, we can train to the 94\% accuracy threshold of the DAWNBench competition in 79 seconds on a single V100 GPU.},
	language = {en-US},
	urldate = {2023-04-24},
	journal = {Myrtle},
	author = {Hay+Rice},
	month = sep,
	year = {2018},
	file = {Snapshot:/Users/cmitra/Zotero/storage/EF65KQGF/how-to-train-your-resnet.html:text/html},
}
